import os
import io
import base64
import torch
import glob
import numpy as np
import logging
from contextlib import asynccontextmanager
from typing import Optional, Dict, Any

# Set up logging FIRST
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suppress noisy warnings while keeping important ones
import warnings
warnings.filterwarnings("ignore", message=".*torch.nn.modules.module.*", category=UserWarning)
warnings.filterwarnings("ignore", message=".*torch.nn.functional.*", category=UserWarning)
warnings.filterwarnings("ignore", message=".*deprecated.*", category=UserWarning)

# Check for advanced optimizations
SAGEATTENTION_AVAILABLE = False
TRITON_AVAILABLE = False

try:
    import sageattention
    SAGEATTENTION_AVAILABLE = True
    logger.info("âœ… SageAttention available - enhanced RTX 5090 optimizations enabled")
except ImportError:
    logger.info("âš ï¸ SageAttention not installed - some RTX 5090 optimizations unavailable")

try:
    import triton
    TRITON_AVAILABLE = True
    logger.info("âœ… Triton available - advanced GPU optimizations enabled")
except ImportError:
    logger.info("âš ï¸ Triton not installed - some GPU optimizations unavailable")

# Import diffusers and other dependencies
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import StableDiffusionXLPipeline
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
from diffusers.schedulers.scheduling_euler_ancestral_discrete import EulerAncestralDiscreteScheduler
from diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_lms_discrete import LMSDiscreteScheduler
from PIL import Image

# DATABASE SERVICES - Replace hardcoded configurations
from database.services import (
    ModelManager, LoraManager, VaeManager, 
    UpscalerManager, EmbeddingManager
)
from database.sqlalchemy_client import get_db_client, LoraService
from database.sqlalchemy_models import (
    Lora, LoraCompatibility, AiModel, Vae, VaeType, 
    Upscaler, UpscalerType, Sampler, SystemConfig
)
from sqlalchemy import select, func, delete
from sqlalchemy.orm import selectinload
import uuid
from assets.samplers import SamplerManager
from database.prompt_service import FavoritePromptService

# Flux support removed - focusing only on non-gated SDXL models
FLUX_AVAILABLE = False

# Global variables
pipeline = None
current_model = None
current_model_type = None
MODEL_DIR = "models"
LORA_DIR = "loras"  # LoRA directory
EMBEDDINGS_DIR = "embeddings"  # Embeddings directory
VAE_DIR = "vaes"  # VAE directory
UPSCALER_DIR = "upscalers"  # Upscaler directory

# ðŸ—„ï¸ DATABASE MANAGERS
model_manager = ModelManager()
lora_manager = LoraManager()
vae_manager = VaeManager()
upscaler_manager = UpscalerManager()
embedding_manager = EmbeddingManager()
sampler_manager = SamplerManager()
prompt_service = FavoritePromptService()

# ðŸ—„ï¸ PURE DATABASE-DRIVEN CACHE
_cached_models = []
_cached_loras = []
_cached_vaes = []
_cached_upscalers = []
_cached_embeddings = []

def get_model_base_classification(model_filename):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get model base classification from database
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup
    global _cached_models
    try:
        if '_cached_models' in globals() and _cached_models:
            for model in _cached_models:
                if model['filename'] == model_filename:
                    return model.get('base_model', 'sdxl_base')
        logger.warning(f"Model {model_filename} not found in cache")
        return 'sdxl_base'  # Default fallback
    except Exception as e:
        logger.error(f"Error accessing cached model data for classification: {e}")
        return 'sdxl_base'

def get_lora_base_classification(lora_filename):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get LoRA compatibility from database
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup
    global _cached_loras
    try:
        if '_cached_loras' in globals() and _cached_loras:
            for lora in _cached_loras:
                if lora['filename'] == lora_filename:
                    return lora.get('compatible_bases', ['sdxl_base'])
        logger.warning(f"LoRA {lora_filename} not found in cache")
        return ['sdxl_base']  # Default fallback
    except Exception as e:
        logger.error(f"Error accessing cached LoRA data for classification: {e}")
        return ['sdxl_base']

# ðŸ—„ï¸ SAMPLERS now database-driven via SamplerManager

async def get_available_models_async():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available models from database (async version)
    For use within async contexts like lifespan - PURE DATABASE APPROACH
    """
    try:
        models = await model_manager.get_available_models()
        return models
    except Exception as e:
        logger.error(f"Database lookup failed for models: {e}")
        return []

def get_available_models():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available models from database (sync version)
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup to avoid async context issues
    global _cached_models
    try:
        if '_cached_models' in globals() and _cached_models:
            return _cached_models
        else:
            logger.warning("No cached model data available - database cache not populated yet")
            return []
    except Exception as e:
        logger.error(f"Error accessing cached model data: {e}")
        return []

async def get_scheduler_async(sampler_name, original_scheduler_config):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get scheduler from database (async version)
    For use within async contexts
    """
    try:
        samplers = await sampler_manager.get_available_samplers()
        
        # Find the sampler in the database
        sampler_info = None
        for sampler in samplers:
            if sampler['name'] == sampler_name:
                sampler_info = sampler
                break
        
        if not sampler_info:
            logger.warning(f"Unknown sampler: {sampler_name}, using DPM++ 2M SDE Karras")
            # Find default sampler
            for sampler in samplers:
                if sampler['name'] == "DPM++ 2M SDE Karras":
                    sampler_info = sampler
                    break
        
        if not sampler_info:
            raise Exception("No samplers available")
        
        # Get scheduler class and config
        scheduler_class = sampler_info["scheduler_class"]
        config = sampler_info["config"]
        
        # Merge original config with sampler-specific config
        merged_config = {**original_scheduler_config, **config}
        
        return scheduler_class.from_config(merged_config)
        
    except Exception as e:
        logger.warning(f"Database sampler lookup failed: {e}")
        # Fallback to DPM++ 2M SDE Karras
        from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
        fallback_config = {
            "algorithm_type": "sde-dpmsolver++",
            "use_karras_sigmas": True,
            "solver_order": 2
        }
        merged_config = {**original_scheduler_config, **fallback_config}
        return DPMSolverMultistepScheduler.from_config(merged_config)

def get_scheduler(sampler_name, original_scheduler_config):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get scheduler from database (sync version)
    For use in non-async contexts
    """
    try:
        # Use sync version to avoid async context issues
        samplers = sampler_manager.get_available_samplers_sync()
        
        # Find the sampler in the cached/fallback samplers
        sampler_info = None
        for sampler in samplers:
            if sampler['name'] == sampler_name:
                sampler_info = sampler
                break
        
        if not sampler_info:
            logger.warning(f"Unknown sampler: {sampler_name}, using DPM++ 2M SDE Karras")
            # Find default sampler
            for sampler in samplers:
                if sampler['name'] == "DPM++ 2M SDE Karras":
                    sampler_info = sampler
                    break
        
        if not sampler_info:
            raise Exception("No samplers available")
        
        # Get scheduler class and config
        scheduler_class_name = sampler_info["scheduler_class"]
        config = sampler_info["config"]
        
        # Import the scheduler class dynamically
        if scheduler_class_name == "DPMSolverMultistepScheduler":
            from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
            scheduler_class = DPMSolverMultistepScheduler
        elif scheduler_class_name == "EulerAncestralDiscreteScheduler":
            from diffusers.schedulers.scheduling_euler_ancestral_discrete import EulerAncestralDiscreteScheduler
            scheduler_class = EulerAncestralDiscreteScheduler
        elif scheduler_class_name == "EulerDiscreteScheduler":
            from diffusers.schedulers.scheduling_euler_discrete import EulerDiscreteScheduler
            scheduler_class = EulerDiscreteScheduler
        elif scheduler_class_name == "DDIMScheduler":
            from diffusers.schedulers.scheduling_ddim import DDIMScheduler
            scheduler_class = DDIMScheduler
        elif scheduler_class_name == "LMSDiscreteScheduler":
            from diffusers.schedulers.scheduling_lms_discrete import LMSDiscreteScheduler
            scheduler_class = LMSDiscreteScheduler
        else:
            # Default fallback
            from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
            scheduler_class = DPMSolverMultistepScheduler
        
        # Merge original config with sampler-specific config
        merged_config = {**original_scheduler_config, **config}
        
        return scheduler_class.from_config(merged_config)
        
    except Exception as e:
        logger.warning(f"Sampler lookup failed: {e}")
        # Fallback to DPM++ 2M SDE Karras
        from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
        fallback_config = {
            "algorithm_type": "sde-dpmsolver++",
            "use_karras_sigmas": True,
            "solver_order": 2
        }
        merged_config = {**original_scheduler_config, **fallback_config}
        return DPMSolverMultistepScheduler.from_config(merged_config)

def load_model(model_path, model_type="sdxl", use_fp8=False):
    """Load SDXL or Flux model with RTX 5090 optimizations and real FP8 support"""
    global pipeline, current_model, current_model_type
    
    try:
        logger.info(f"Loading {model_type.upper()} model: {model_path}")
        logger.info(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name()}")
        
        # Clear existing pipeline with aggressive memory cleanup
        if pipeline is not None:
            logger.info("ðŸ§¹ Clearing existing pipeline from VRAM...")
            del pipeline
            
            # Force garbage collection
            import gc
            gc.collect()
            
            # Multiple cache clears for stubborn memory
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # Wait for all operations to complete
                torch.cuda.empty_cache()  # Second pass
                
                # Log memory usage
                allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                cached = torch.cuda.memory_reserved() / 1024**3     # GB
                logger.info(f"ðŸ“Š VRAM after cleanup: {allocated:.1f}GB allocated, {cached:.1f}GB cached")
        
        # Load SDXL model from local file
        logger.info(f"Loading SDXL model using StableDiffusionXLPipeline...")
        try:
            pipeline = StableDiffusionXLPipeline.from_single_file(
                model_path,
                torch_dtype=torch.float16,
                use_safetensors=True,
                config="stabilityai/stable-diffusion-xl-base-1.0"  # Use base config
            )
        except Exception as e:
            logger.warning(f"Standard SDXL loading failed: {e}, trying alternative method")
            # Alternative loading method
            pipeline = StableDiffusionXLPipeline.from_single_file(
                model_path,
                torch_dtype=torch.float16,
                use_safetensors=True
            )
        
        # Set default scheduler for SDXL models to DPM++ 2M SDE Karras
        try:
            # Try database lookup first
            default_scheduler = get_scheduler("DPM++ 2M SDE Karras", pipeline.scheduler.config)
            pipeline.scheduler = default_scheduler
            logger.info("Set default scheduler: DPM++ 2M SDE Karras (from database)")
        except Exception as e:
            # Fallback to hardcoded DPM++ 2M SDE Karras if database lookup fails
            logger.warning(f"Database scheduler lookup failed: {e}, using hardcoded fallback")
            from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler
            try:
                original_config = pipeline.scheduler.config  # type: ignore
                fallback_config = {
                    **original_config,
                    "algorithm_type": "sde-dpmsolver++",
                    "use_karras_sigmas": True,
                    "solver_order": 2
                }
            except:
                # If config access fails, use minimal config
                fallback_config = {
                    "algorithm_type": "sde-dpmsolver++",
                    "use_karras_sigmas": True,
                    "solver_order": 2
                }
            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(fallback_config)
            logger.info("Set default scheduler: DPM++ 2M SDE Karras (hardcoded fallback)")
        
        if torch.cuda.is_available():
            pipeline = pipeline.to("cuda")
            
            # RTX 5090 OPTIMIZATIONS - Enable all Hopper architecture features
            gpu_name = torch.cuda.get_device_name().lower()
            is_rtx_5090 = "rtx 5090" in gpu_name or "rtx5090" in gpu_name
            is_hopper = is_rtx_5090  # Future-proof for other Hopper GPUs
            
            if is_rtx_5090:
                logger.info("ðŸš€ RTX 5090 detected! Enabling maximum Hopper optimizations")
            
            # Apply SDXL attention optimizations
            try:
                # SDXL models use UNet architecture
                if hasattr(pipeline, 'unet') and hasattr(pipeline.unet, 'set_attn_processor'):
                    # Use Flash Attention on UNet
                    from diffusers.models.attention_processor import AttnProcessor2_0
                    pipeline.unet.set_attn_processor(AttnProcessor2_0())
                    logger.info("âœ… SDXL: PyTorch native Flash Attention enabled on UNet")
                
                # Enable scaled dot-product attention backends for maximum performance
                if is_hopper:
                    try:
                        torch.backends.cuda.enable_flash_sdp(True)
                        torch.backends.cuda.enable_mem_efficient_sdp(True)
                        torch.backends.cuda.enable_math_sdp(True)
                        logger.info("ðŸš€ SDXL: Scaled dot-product attention backends enabled")
                    except Exception as e:
                        logger.warning(f"Could not enable SDPA backends: {e}")
                elif hasattr(pipeline, 'enable_xformers_memory_efficient_attention'):
                    # Fallback to XFormers for other GPUs
                    pipeline.enable_xformers_memory_efficient_attention()
                    logger.info("âœ… SDXL: XFormers memory efficient attention enabled")
                else:
                    logger.info("â„¹ï¸ Using model's default attention implementation")
                        
            except Exception as e:
                logger.warning(f"Could not enable attention optimization: {e}")
                logger.info("âš ï¸ Using standard attention (will be slower)")
            
            # Enable VAE optimizations for memory efficiency
            if hasattr(pipeline, 'enable_vae_slicing'):
                pipeline.enable_vae_slicing()
                logger.info("âœ… VAE slicing enabled")
            if hasattr(pipeline, 'enable_vae_tiling'):
                pipeline.enable_vae_tiling()
                logger.info("âœ… VAE tiling enabled")
            
            # REAL FP8 OPTIMIZATION - Use actual torch.float8_e4m3fn dtype
            if use_fp8:
                logger.info("ðŸ”¥ Attempting REAL FP8 optimization...")
                
                # Check FP8 capability first
                fp8_supported = False
                try:
                    # Test if FP8 dtype is available
                    test_tensor = torch.tensor([1.0], device="cuda")
                    test_fp8 = test_tensor.to(dtype=torch.float8_e4m3fn)
                    # Test basic operations
                    result = test_fp8 + test_fp8
                    result = result.to(dtype=torch.float16)  # Convert back
                    fp8_supported = True
                    logger.info("âœ… FP8 capability check passed")
                except Exception as e:
                    logger.warning(f"âš ï¸ FP8 capability check failed: {e}")
                    fp8_supported = False
                
                if fp8_supported:
                    try:
                        # SDXL models use UNet
                        if hasattr(pipeline, 'unet'):
                            pipeline.unet.to(dtype=torch.float8_e4m3fn)
                            logger.info("âœ… SDXL UNet cast to FP8 precision")
                        
                        # Note: Text encoders and VAE may not support FP8 operations
                        logger.info("ðŸ”¥ FP8 precision enabled for SDXL model")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ FP8 casting failed: {e}, reverting to FP16")
                        # Revert to FP16 if FP8 fails
                        if hasattr(pipeline, 'unet'):
                            pipeline.unet.to(dtype=torch.float16)
                        fp8_supported = False
                
                if not fp8_supported:
                    # Fallback optimizations when FP8 isn't available
                    logger.info("ðŸ”„ Using optimized FP16/TF32 fallback")
                    try:
                        # Use torch.compile for optimization
                        if hasattr(torch, 'compile') and hasattr(pipeline, 'unet'):
                            pipeline.unet = torch.compile(pipeline.unet, mode="max-autotune")
                            logger.info("âœ… UNet compiled with max optimization")
                    except Exception as e:
                        logger.warning(f"Could not compile UNet: {e}")
            
            # RTX 5090 Memory and computation optimizations
            if torch.cuda.is_available():
                try:
                    # Enable memory pool for better allocation
                    torch.cuda.empty_cache()
                    torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
                    torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for speed
                    torch.backends.cudnn.allow_tf32 = True
                    
                    if is_hopper and use_fp8:
                        # Additional optimizations for FP8 on Hopper
                        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
                        logger.info("ðŸš€ Hopper FP8 optimizations enabled (TF32, FP16 reduction)")
                    else:
                        logger.info("ðŸš€ CUDA optimizations enabled (TF32, cuDNN benchmark)")
                except Exception as e:
                    logger.warning(f"Could not enable CUDA optimizations: {e}")
        
        # Set model name and type
        current_model = os.path.basename(model_path)
        current_model_type = model_type
        
        # Log final memory usage and optimization summary
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3     # GB
            logger.info(f"ðŸ“Š VRAM after loading SDXL: {allocated:.1f}GB allocated, {cached:.1f}GB cached")
            
            # RTX 5090 optimization summary
            gpu_name = torch.cuda.get_device_name()
            if "RTX 5090" in gpu_name or "RTX5090" in gpu_name:
                logger.info("ðŸš€ RTX 5090 OPTIMIZATION SUMMARY (SDXL):")
                logger.info("   âœ… Hopper architecture optimizations enabled")
                logger.info("   âœ… TF32 and cuDNN benchmark enabled")
                logger.info("   âœ… SDXL Flash Attention / SDPA backends enabled")
                if use_fp8:
                    logger.info("   ðŸ”¥ Real FP8 precision active on UNet!")
                else:
                    logger.info("   ðŸ’¡ Use 'use_fp8: true' for maximum SDXL performance!")
                        
                logger.info("   âœ… VAE slicing and tiling enabled")
                logger.info("   ðŸŽ¯ Expected performance: Up to 1.2 PFLOPS with FP8")
        
        logger.info(f"SDXL pipeline loaded successfully: {current_model}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to load {model_type} pipeline: {e}")
        pipeline = None
        current_model = None
        current_model_type = None
        return False

def encode_long_prompt_with_embeddings(pipeline, prompt, device="cuda", clip_skip=None):
    """
    COMPLETE REWRITE: Real sd_embed implementation for SDXL
    Based on actual sd_embed embedding-funcs.py and prompt-parser.py
    
    Key Features:
    - Real prompt parsing with (word:1.2) and [word:0.8] syntax
    - 77-token chunking (75 content + BOS + EOS)
    - Weight application in embedding space
    - Proper hidden states extraction
    - Channel concatenation for SDXL format
    """
    if not prompt.strip():
        prompt = "empty"  # sd_embed default
    
    logger.info(f"ðŸ”¥ REAL SD_EMBED IMPLEMENTATION - Unlimited tokens with perfect semantic preservation!")
    
    tokenizer_1 = pipeline.tokenizer
    tokenizer_2 = pipeline.tokenizer_2
    text_encoder_1 = pipeline.text_encoder
    text_encoder_2 = pipeline.text_encoder_2
    
    def parse_prompt_attention(text):
        """
        Real sd_embed prompt parsing implementation
        Based on sd_embed/prompt_parser.py parse_prompt_attention()
        
        Supports:
        (abc) - increases attention to abc by a multiplier of 1.1
        (abc:3.12) - increases attention to abc by a multiplier of 3.12
        [abc] - decreases attention to abc by a multiplier of 1.1
        """
        import re
        
        re_attention = re.compile(r"""
        \\\(|
        \\\)|
        \\\[|
        \\]|
        \\\\|
        \\|
        \(|
        \[|
        :([+-]?[.\d]+)\)|
        \)|
        ]|
        [^\\()\[\]:]+|
        :
        """, re.X)
        
        res = []
        round_brackets = []
        square_brackets = []
        
        round_bracket_multiplier = 1.1
        square_bracket_multiplier = 1 / 1.1
        
        def multiply_range(start_position, multiplier):
            for p in range(start_position, len(res)):
                res[p][1] *= multiplier
        
        for m in re_attention.finditer(text):
            text_part = m.group(0)
            weight = m.group(1)
            
            if text_part.startswith('\\'):
                res.append([text_part[1:], 1.0])
            elif text_part == '(':
                round_brackets.append(len(res))
            elif text_part == '[':
                square_brackets.append(len(res))
            elif weight is not None and len(round_brackets) > 0:
                multiply_range(round_brackets.pop(), float(weight))
            elif text_part == ')' and len(round_brackets) > 0:
                multiply_range(round_brackets.pop(), round_bracket_multiplier)
            elif text_part == ']' and len(square_brackets) > 0:
                multiply_range(square_brackets.pop(), square_bracket_multiplier)
            else:
                res.append([text_part, 1.0])
        
        for pos in round_brackets:
            multiply_range(pos, round_bracket_multiplier)
        
        for pos in square_brackets:
            multiply_range(pos, square_bracket_multiplier)
        
        if len(res) == 0:
            res = [["", 1.0]]
        
        # merge runs of identical weights
        i = 0
        while i + 1 < len(res):
            if res[i][1] == res[i + 1][1]:
                res[i][0] += res[i + 1][0]
                res.pop(i + 1)
            else:
                i += 1
        
        return res
    
    def get_prompts_tokens_with_weights(clip_tokenizer, prompt_text):
        """
        Real sd_embed tokenization with weights
        Based on sd_embed/embedding_funcs.py get_prompts_tokens_with_weights()
        """
        texts_and_weights = parse_prompt_attention(prompt_text)
        text_tokens, text_weights = [], []
        
        for word, weight in texts_and_weights:
            # tokenize and discard the starting and the ending token
            token = clip_tokenizer(
                word,
                truncation=False  # so that tokenize whatever length prompt
            ).input_ids[1:-1]
            # the returned token is a 1d list: [320, 1125, 539, 320]
            
            # merge the new tokens to the all tokens holder: text_tokens
            text_tokens = [*text_tokens, *token]
            
            # each token chunk will come with one weight, like ['red cat', 2.0]
            # need to expand weight for each token.
            chunk_weights = [weight] * len(token)
            
            # append the weight back to the weight holder: text_weights
            text_weights = [*text_weights, *chunk_weights]
        
        return text_tokens, text_weights
    
    def group_tokens_and_weights(token_ids, weights, pad_last_block=True):
        """
        Real sd_embed chunking implementation
        Based on sd_embed/embedding_funcs.py group_tokens_and_weights()
        
        Creates 77-token groups: BOS + 75 content tokens + EOS
        """
        bos, eos = 49406, 49407
        
        # this will be a 2d list
        new_token_ids = []
        new_weights = []
        
        while len(token_ids) >= 75:
            # get the first 75 tokens
            head_75_tokens = [token_ids.pop(0) for _ in range(75)]
            head_75_weights = [weights.pop(0) for _ in range(75)]
            
            # extract token ids and weights
            temp_77_token_ids = [bos] + head_75_tokens + [eos]
            temp_77_weights = [1.0] + head_75_weights + [1.0]
            
            # add 77 token and weights chunk to the holder list
            new_token_ids.append(temp_77_token_ids)
            new_weights.append(temp_77_weights)
        
        # padding the left
        if len(token_ids) > 0:
            padding_len = 75 - len(token_ids) if pad_last_block else 0
            
            temp_77_token_ids = [bos] + token_ids + [eos] * padding_len + [eos]
            new_token_ids.append(temp_77_token_ids)
            
            temp_77_weights = [1.0] + weights + [1.0] * padding_len + [1.0]
            new_weights.append(temp_77_weights)
        
        return new_token_ids, new_weights
    
    def get_prompt_hidden_states_sdxl(prompt_embeds, clip_skip=None):
        """
        Real sd_embed hidden states extraction
        "'2' because SDXL always indexes from the penultimate layer."
        """
        final_layer_index = 2
        if clip_skip is None:
            return prompt_embeds.hidden_states[-(final_layer_index)]
        return prompt_embeds.hidden_states[-(clip_skip + final_layer_index)]
    
    def get_weighted_text_embeddings_sdxl_single_encoder(tokenizer, text_encoder, prompt_text, clip_skip=None):
        """
        Real sd_embed SDXL processing for a single encoder with CLIP skip support
        Based on sd_embed/embedding_funcs.py get_weighted_text_embeddings_sdxl()
        """
        eos = tokenizer.eos_token_id
        
        # Get tokens and weights using real sd_embed approach
        prompt_tokens, prompt_weights = get_prompts_tokens_with_weights(tokenizer, prompt_text)
        
        logger.info(f"   ðŸ“Š Parsed {len(prompt_tokens)} tokens with weights from prompt")
        
        # Group tokens into 77-token chunks (75 content + BOS + EOS)
        prompt_token_groups, prompt_weight_groups = group_tokens_and_weights(
            prompt_tokens.copy(), prompt_weights.copy(), pad_last_block=True
        )
        
        num_chunks = len(prompt_token_groups)
        logger.info(f"   ðŸ“¦ Created {num_chunks} 77-token groups (75 content + BOS + EOS)")
        
        embeds = []
        pooled_embeds = []
        
        # Process each 77-token group
        for i in range(len(prompt_token_groups)):
            # Convert to tensor
            token_tensor = torch.tensor(
                [prompt_token_groups[i]],
                dtype=torch.long, device=device
            )
            weight_tensor = torch.tensor(
                prompt_weight_groups[i],
                dtype=torch.float16, device=device
            )
            
            # Encode with text encoder
            with torch.no_grad():
                outputs = text_encoder(
                    token_tensor.to(device),
                    output_hidden_states=True
                )
            
            # Extract hidden states with CLIP skip support
            if hasattr(outputs, 'hidden_states'):
                hidden_states = get_prompt_hidden_states_sdxl(outputs, clip_skip=clip_skip)
            else:
                hidden_states = outputs.last_hidden_state
            
            token_embedding = hidden_states.squeeze(0)
            
            # Apply weights in embedding space (real sd_embed approach!)
            for j in range(len(weight_tensor)):
                if weight_tensor[j] != 1.0:
                    token_embedding[j] = token_embedding[j] * weight_tensor[j]
            
            token_embedding = token_embedding.unsqueeze(0)
            embeds.append(token_embedding)
            
            # Save pooled output from first chunk for text_encoder_2
            if i == 0 and hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                pooled_embeds.append(outputs.pooler_output)
        
        # Concatenate all chunks along sequence dimension
        final_embeddings = torch.cat(embeds, dim=1)
        final_pooled = pooled_embeds[0] if pooled_embeds else None
        
        logger.info(f"   âœ… Processed {num_chunks} chunks â†’ {final_embeddings.shape}")
        
        return final_embeddings, final_pooled, num_chunks
    
    # Process both SDXL text encoders using real sd_embed approach
    logger.info(f"   ðŸŽ¯ Processing with text_encoder_1 (CLIP ViT-L, 768 dims)...")
    embeds_1, pooled_1, chunks_1 = get_weighted_text_embeddings_sdxl_single_encoder(
        tokenizer_1, text_encoder_1, prompt, clip_skip=clip_skip
    )
    
    logger.info(f"   ðŸŽ¯ Processing with text_encoder_2 (OpenCLIP ViT-bigG, 1280 dims)...")  
    embeds_2, pooled_2, chunks_2 = get_weighted_text_embeddings_sdxl_single_encoder(
        tokenizer_2, text_encoder_2, prompt, clip_skip=clip_skip
    )
    
    # Ensure both encoders have same sequence length
    if embeds_1.shape[1] != embeds_2.shape[1]:
        logger.warning(f"Sequence length mismatch: {embeds_1.shape[1]} vs {embeds_2.shape[1]}")
        max_seq_len = max(embeds_1.shape[1], embeds_2.shape[1])
        
        if embeds_1.shape[1] < max_seq_len:
            pad_size = max_seq_len - embeds_1.shape[1]
            padding = torch.zeros(1, pad_size, embeds_1.shape[2], device=embeds_1.device, dtype=embeds_1.dtype)
            embeds_1 = torch.cat([embeds_1, padding], dim=1)
        
        if embeds_2.shape[1] < max_seq_len:
            pad_size = max_seq_len - embeds_2.shape[1]
            padding = torch.zeros(1, pad_size, embeds_2.shape[2], device=embeds_2.device, dtype=embeds_2.dtype)
            embeds_2 = torch.cat([embeds_2, padding], dim=1)
    
    # SDXL requires pooled embeddings from text_encoder_2 ONLY
    if pooled_2 is None:
        logger.warning("âš ï¸ text_encoder_2 missing pooled embeddings - creating fallback")
        pooled_2 = torch.zeros((1, text_encoder_2.config.projection_dim), device=device, dtype=torch.float16)
    
    max_chunks = max(chunks_1, chunks_2)
    logger.info(f"âœ¨ REAL SD_EMBED IMPLEMENTATION COMPLETE!")
    logger.info(f"   ðŸ“Š Results: embeds_1={embeds_1.shape}, embeds_2={embeds_2.shape}, pooled={pooled_2.shape}")
    logger.info(f"   ðŸŽ¯ Total chunks processed: {max_chunks}")
    logger.info(f"   ðŸŽ¯ CLIP Skip setting: {clip_skip or 'Default (model natural behavior)'}")
    logger.info(f"   ðŸ”§ Ready for CHANNEL concatenation: {embeds_1.shape} + {embeds_2.shape} = SDXL format")
    logger.info(f"   âœ… Features: Real prompt parsing, 77-token chunking, weight application in embedding space, CLIP skip support")
    
    return embeds_1, embeds_2, pooled_2, max_chunks

def get_lora_guidelines_from_filename(filename):
    """
    Get LoRA guidelines based on filename patterns and creator recommendations
    
    Args:
        filename: LoRA filename to analyze
    
    Returns:
        dict: Guidelines with flexible ranges based on LoRA type
    """
    filename_lower = filename.lower()
    
    # Universal default - FULL range available to ALL LoRAs
    guidelines = {
        "recommended_scale": 1.0,
        "scale_range": {"min": -5.0, "max": 5.0},  # Full range for maximum flexibility
        "description": "LoRA - typical range 0.5-1.5, full range available",
        "category": "general"
    }
    
    # SPECIFIC CONFIGURATIONS - Only for explicitly requested patterns
    
    # Detail Enhancement LoRAs (like "more_details", "detail_tweaker") - based on creator guidelines
    if any(word in filename_lower for word in ["detail", "details"]):
        guidelines.update({
            "recommended_scale": 0.7,  # Between 0.5-1.0 range, avoiding the "too high" 1.0 default
            "scale_range": {"min": -5.0, "max": 5.0},  # Full range still available
            "description": "Detail enhancer - use 0.5-1.0 range, can go lower for subtle effects, negative values for interesting results",
            "category": "detail_enhancement"
        })
    
    # Skin tone sliders - specific pattern for skin tone adjustment
    elif any(word in filename_lower for word in ["skin_tone", "skintone"]) and "slider" in filename_lower:
        guidelines.update({
            "recommended_scale": 0.0,  # Start neutral
            "scale_range": {"min": -5.0, "max": 5.0},
            "description": "Skin tone slider - supports -5 to +5 range. Positive gives darker skin, negative gives lighter skin",
            "category": "skin_tone_slider"
        })
    
    return guidelines

async def get_available_loras_async(filter_by_current_model=False):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available LoRAs from database (async version)
    For use within async contexts
    """
    try:
        loras = await lora_manager.get_available_loras(
            filter_by_current_model=filter_by_current_model,
            current_model_filename=current_model if filter_by_current_model else None
        )
        
        return loras
        
    except Exception as e:
        logger.warning(f"Database LoRA lookup failed: {e}")
        # Fallback to filesystem scan if database fails
        loras = []
        if os.path.exists(LORA_DIR):
            lora_files = glob.glob(os.path.join(LORA_DIR, "*.safetensors"))
            for file_path in lora_files:
                filename = os.path.basename(file_path)
                display_name = filename.replace(".safetensors", "").replace("_", " ").replace("-", " ")
                loras.append({
                    "filename": filename,
                    "display_name": display_name,
                    "path": file_path,
                    "type": "lora",
                    "source": "local"
                })
        return loras

def get_available_loras(filter_by_current_model=False):
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available LoRAs from database (sync version)
    REPLACES: Hardcoded filesystem scanning with NSFW LoRA references
    """
    try:
        # Fallback to filesystem scan for sync version to avoid async context issues
        logger.warning("Sync version called - using filesystem scan fallback")
        loras = []
        if os.path.exists(LORA_DIR):
            lora_files = glob.glob(os.path.join(LORA_DIR, "*.safetensors"))
            for file_path in lora_files:
                filename = os.path.basename(file_path)
                display_name = filename.replace(".safetensors", "").replace("_", " ").replace("-", " ")
                loras.append({
                    "filename": filename,
                    "display_name": display_name,
                    "path": file_path,
                    "type": "lora",
                    "source": "local"
                })
        
        # Apply filtering if requested
        if filter_by_current_model and current_model:
            current_model_base = get_model_base_classification(current_model)
            logger.info(f"ðŸŽ¨ Filtering LoRAs for current model base: {current_model_base}")
            
            filtered_loras = []
            for lora in loras:
                compatible_bases = lora.get('compatible_bases', ['sdxl_base'])
                if current_model_base in compatible_bases:
                    filtered_loras.append(lora)
                else:
                    logger.info(f"   ðŸš« Filtered out {lora['display_name']}: requires {'/'.join(compatible_bases)}, current model is {current_model_base}")
            
            return filtered_loras
        
        return loras
        
    except Exception as e:
        logger.warning(f"LoRA lookup failed: {e}")
        # Fallback to filesystem scan if database fails
        loras = []
        if os.path.exists(LORA_DIR):
            lora_files = glob.glob(os.path.join(LORA_DIR, "*.safetensors"))
            for file_path in lora_files:
                filename = os.path.basename(file_path)
                display_name = filename.replace(".safetensors", "").replace("_", " ").replace("-", " ")
                loras.append({
                    "filename": filename,
                    "display_name": display_name,
                    "path": file_path,
                    "type": "lora",
                    "source": "local"
                })
        return loras

def load_multiple_loras_to_pipeline(pipeline, loras_to_load):
    """
    Load multiple LoRAs into the pipeline with proper error handling and compatibility checking
    
    Args:
        pipeline: The diffusers pipeline
        loras_to_load: List of {"path": str, "scale": float, "name": str} dicts
    
    Returns:
        bool: Success status
    """
    try:
        if not loras_to_load:
            return True
        
        logger.info(f"ðŸŽ¨ Loading {len(loras_to_load)} LoRA(s) with compatibility checking")
        
        # First unload any existing LoRAs
        unload_loras_from_pipeline(pipeline)
        
        # Collect successfully loaded adapters
        adapter_names = []
        adapter_scales = []
        successfully_loaded = []
        
        # Load each LoRA individually with error handling
        for i, lora_info in enumerate(loras_to_load):
            lora_path = lora_info["path"]
            lora_scale = lora_info["scale"]
            lora_name = lora_info["name"]
            
            # Create unique adapter name (sanitize for safety)
            adapter_name = f"lora_{i}_{lora_name.replace(' ', '_').replace('.', '_').replace('-', '_')}"
            
            logger.info(f"   ðŸ“¦ Loading LoRA {i+1}/{len(loras_to_load)}: {lora_name} (scale: {lora_scale})")
            
            # Check compatibility first
            is_compatible, compat_reason = check_lora_compatibility(lora_path, pipeline)
            if not is_compatible:
                logger.warning(f"   âš ï¸ Skipping incompatible LoRA '{lora_name}': {compat_reason}")
                continue
            else:
                logger.info(f"   ðŸ” LoRA compatibility: {compat_reason}")
            
            try:
                # Try to load LoRA with specific adapter name
                pipeline.load_lora_weights(lora_path, adapter_name=adapter_name)
                
                adapter_names.append(adapter_name)
                adapter_scales.append(lora_scale)
                successfully_loaded.append(lora_name)
                
                logger.info(f"   âœ… Successfully loaded: {lora_name}")
                
            except Exception as lora_error:
                error_msg = str(lora_error)
                if "size mismatch" in error_msg.lower():
                    logger.warning(f"   âš ï¸ LoRA '{lora_name}' has incompatible dimensions - likely for different model architecture")
                elif "tensor" in error_msg.lower() and "not found" in error_msg.lower():
                    logger.warning(f"   âš ï¸ LoRA '{lora_name}' missing expected tensors - may be corrupted or wrong format")
                else:
                    logger.warning(f"   âš ï¸ Failed to load LoRA '{lora_name}': {error_msg[:100]}...")
                # Continue with other LoRAs even if one fails
                continue
        
        # Apply scales only to successfully loaded LoRAs
        if adapter_names and hasattr(pipeline, 'set_adapters'):
            try:
                pipeline.set_adapters(adapter_names, adapter_weights=adapter_scales)
                logger.info(f"   âœ… Applied {len(adapter_names)} LoRA adapters with scales: {adapter_scales}")
            except Exception as adapter_error:
                logger.warning(f"   âš ï¸ Failed to set adapter scales: {adapter_error}")
                # LoRAs are loaded, but scales might not apply correctly
        elif adapter_names:
            logger.warning("   âš ï¸ Pipeline doesn't support set_adapters - LoRAs loaded but scales may not apply correctly")
        
        if successfully_loaded:
            logger.info(f"ðŸŽ¨ Successfully loaded {len(successfully_loaded)} LoRA(s): {', '.join(successfully_loaded)}")
            return True
        else:
            logger.warning("ðŸŽ¨ No LoRAs were successfully loaded")
            return False
        
    except Exception as e:
        logger.error(f"âŒ Critical error in LoRA loading: {e}")
        # Try to clean up any partially loaded state
        try:
            unload_loras_from_pipeline(pipeline)
        except:
            pass
        return False

def unload_loras_from_pipeline(pipeline):
    """
    Unload all LoRAs from the pipeline to clean state with robust error handling
    """
    try:
        success = True
        
        # Reset adapter configuration first
        if hasattr(pipeline, 'set_adapters'):
            try:
                pipeline.set_adapters([])  # Clear all adapters
                logger.info("ðŸ§¹ LoRA adapters cleared")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not clear adapters: {e}")
                success = False
        
        # Unload LoRA weights
        if hasattr(pipeline, 'unload_lora_weights'):
            try:
                pipeline.unload_lora_weights()
                logger.info("ðŸ§¹ LoRA weights unloaded")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not unload LoRA weights: {e}")
                success = False
        
        # Additional cleanup - disable LoRA if possible
        if hasattr(pipeline, 'disable_lora'):
            try:
                pipeline.disable_lora()
                logger.info("ðŸ§¹ LoRA disabled")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not disable LoRA: {e}")
        
        if success:
            logger.info("ðŸ§¹ All LoRAs successfully unloaded from pipeline")
        else:
            logger.warning("ðŸ§¹ LoRAs partially unloaded (some operations failed)")
            
        return success
        
    except Exception as e:
        logger.error(f"âš ï¸ Critical error unloading LoRAs: {e}")
        return False

def check_lora_compatibility(lora_path, pipeline):
    """
    Enhanced compatibility check for LoRA files including base model matching
    
    Args:
        lora_path: Path to LoRA file
        pipeline: Current pipeline
    
    Returns:
        tuple: (is_compatible, reason)
    """
    try:
        from safetensors import safe_open
        
        # Get LoRA filename for base model classification
        lora_filename = os.path.basename(lora_path)
        compatible_bases = get_lora_base_classification(lora_filename)
        
        # Check base model compatibility with current model
        if current_model:
            current_base = get_model_base_classification(current_model)
            if current_base not in compatible_bases:
                return False, f"LoRA is for {'/'.join(compatible_bases)} models, current model is {current_base}"
        
        # Load LoRA metadata to check technical compatibility
        with safe_open(lora_path, framework="pt", device="cpu") as f:
            metadata = f.metadata()
            
            # Check if it's an SDXL LoRA
            if metadata:
                # Look for SDXL-specific indicators
                model_type = metadata.get("ss_base_model_version", "").lower()
                if "sdxl" in model_type:
                    return True, f"SDXL LoRA compatible with {'/'.join(compatible_bases)} base models"
                elif "sd_xl" in model_type:
                    return True, f"SD-XL LoRA compatible with {'/'.join(compatible_bases)} base models"
                elif "1.5" in model_type or "sd1" in model_type:
                    return False, "SD 1.5 LoRA (incompatible with SDXL)"
            
            # If no clear metadata, assume compatible if base model matches
            return True, f"Base model compatible ({'/'.join(compatible_bases)})"
            
    except Exception as e:
        # If we can't check, assume compatible and let loading handle it
        return True, f"Could not check compatibility: {e}"

async def get_available_embeddings_async():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available embeddings from database (async version)
    For use within async contexts
    """
    try:
        embeddings = await embedding_manager.get_available_embeddings()
        return embeddings
    except Exception as e:
        logger.warning(f"Database embedding lookup failed: {e}")
        # Fallback to filesystem scan if database fails
        embeddings = []
        if os.path.exists(EMBEDDINGS_DIR):
            file_patterns = ["*.safetensors", "*.pt", "*.bin"]
            for pattern in file_patterns:
                embedding_files = glob.glob(os.path.join(EMBEDDINGS_DIR, "**", pattern), recursive=True)
                for file_path in embedding_files:
                    filename = os.path.basename(file_path)
                    trigger_word = os.path.splitext(filename)[0]
                    embeddings.append({
                        "filename": filename,
                        "trigger_word": trigger_word,
                        "path": file_path,
                        "category": "general",
                        "type": "general",
                        "source": "local"
                    })
        return embeddings

def get_available_embeddings():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available embeddings from database (sync version)
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup to avoid async context issues
    global _cached_embeddings
    try:
        if '_cached_embeddings' in globals() and _cached_embeddings:
            return _cached_embeddings
        else:
            logger.warning("No cached embedding data available - database cache not populated yet")
            return []
    except Exception as e:
        logger.error(f"Error accessing cached embedding data: {e}")
        return []

async def load_embeddings_to_pipeline_async(pipeline, embeddings_to_load=None):
    """
    Load embeddings into the pipeline's text encoders (async version)
    For use within async contexts
    """
    try:
        logger.info("ðŸ“š Loading embeddings into text encoders...")
        
        # Get available embeddings
        available_embeddings = await get_available_embeddings_async()
        
        if not available_embeddings:
            logger.info("ðŸ“š No embeddings found to load")
            return {"status": "success", "loaded": 0, "embeddings": []}
        
        # Determine which embeddings to load
        if embeddings_to_load is None:
            # Load all available embeddings
            to_load = available_embeddings
        else:
            # Load only specified embeddings
            to_load = [emb for emb in available_embeddings if emb["path"] in embeddings_to_load]
        
        loaded_embeddings = []
        
        for embedding_info in to_load:
            try:
                embedding_path = embedding_info["path"]
                trigger_word = embedding_info["trigger_word"]
                
                logger.info(f"ðŸ“š Loading embedding: {trigger_word} from {embedding_path}")
                
                # Use diffusers textual inversion loading
                # This loads the embedding into the tokenizer and text encoder
                pipeline.load_textual_inversion(embedding_path, token=trigger_word)
                
                loaded_embeddings.append({
                    "trigger_word": trigger_word,
                    "path": embedding_path,
                    "type": embedding_info["type"],
                    "category": embedding_info["category"]
                })
                
                logger.info(f"âœ… Loaded embedding: {trigger_word}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load embedding {embedding_info['trigger_word']}: {e}")
                continue
        
        logger.info(f"ðŸ“š Successfully loaded {len(loaded_embeddings)} embeddings")
        
        return {
            "status": "success",
            "loaded": len(loaded_embeddings),
            "embeddings": loaded_embeddings,
            "available": len(available_embeddings),
            "failed": len(to_load) - len(loaded_embeddings)
        }
        
    except Exception as e:
        logger.error(f"âŒ Error loading embeddings: {e}")
        return {
            "status": "error",
            "message": str(e),
            "loaded": 0,
            "embeddings": []
        }

def load_embeddings_to_pipeline(pipeline, embeddings_to_load=None):
    """
    Load embeddings into the pipeline's text encoders (sync version)
    For use in non-async contexts
    """
    try:
        logger.info("ðŸ“š Loading embeddings into text encoders...")
        
        # Get available embeddings
        available_embeddings = get_available_embeddings()
        
        if not available_embeddings:
            logger.info("ðŸ“š No embeddings found to load")
            return {"status": "success", "loaded": 0, "embeddings": []}
        
        # Determine which embeddings to load
        if embeddings_to_load is None:
            # Load all available embeddings
            to_load = available_embeddings
        else:
            # Load only specified embeddings
            to_load = [emb for emb in available_embeddings if emb["path"] in embeddings_to_load]
        
        loaded_embeddings = []
        
        for embedding_info in to_load:
            try:
                embedding_path = embedding_info["path"]
                trigger_word = embedding_info["trigger_word"]
                
                logger.info(f"ðŸ“š Loading embedding: {trigger_word} from {embedding_path}")
                
                # Use diffusers textual inversion loading
                # This loads the embedding into the tokenizer and text encoder
                pipeline.load_textual_inversion(embedding_path, token=trigger_word)
                
                loaded_embeddings.append({
                    "trigger_word": trigger_word,
                    "path": embedding_path,
                    "type": embedding_info["type"],
                    "category": embedding_info["category"]
                })
                
                logger.info(f"âœ… Loaded embedding: {trigger_word}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load embedding {embedding_info['trigger_word']}: {e}")
                continue
        
        logger.info(f"ðŸ“š Successfully loaded {len(loaded_embeddings)} embeddings")
        
        return {
            "status": "success",
            "loaded": len(loaded_embeddings),
            "embeddings": loaded_embeddings,
            "available": len(available_embeddings),
            "failed": len(to_load) - len(loaded_embeddings)
        }
        
    except Exception as e:
        logger.error(f"âŒ Error loading embeddings: {e}")
        return {
            "status": "error",
            "message": str(e),
            "loaded": 0,
            "embeddings": []
        }

def get_embedding_suggestions_for_text(text):
    """
    Analyze text and suggest relevant embeddings
    
    Args:
        text: The prompt text to analyze
    
    Returns:
        list: Suggested embeddings with reasons
    """
    suggestions = []
    available_embeddings = get_available_embeddings()
    
    text_lower = text.lower()
    
    for embedding in available_embeddings:
        trigger_word = embedding["trigger_word"].lower()
        
        # Check if trigger word is already in text
        if trigger_word in text_lower:
            continue
        
        # Suggest based on embedding type and text content
        if embedding["type"] == "negative":
            # Suggest negative embeddings for negative prompts or quality issues
            if any(word in text_lower for word in ["bad", "low", "ugly", "worst", "blurry", "distorted"]):
                suggestions.append({
                    "embedding": embedding,
                    "reason": "Improves negative prompt quality",
                    "confidence": 0.8
                })
        
        elif embedding["type"] == "quality":
            # Suggest quality embeddings for enhancement
            if any(word in text_lower for word in ["high", "detail", "quality", "sharp", "hd", "masterpiece"]):
                suggestions.append({
                    "embedding": embedding,
                    "reason": "Enhances image quality and detail",
                    "confidence": 0.7
                })
        
        elif embedding["type"] == "style":
            # Suggest style embeddings for artistic prompts
            if any(word in text_lower for word in ["art", "style", "painting", "photo", "realistic", "anime"]):
                suggestions.append({
                    "embedding": embedding,
                    "reason": "Applies artistic style enhancement",
                    "confidence": 0.6
                })
    
    # Sort by confidence and limit results
    suggestions.sort(key=lambda x: x["confidence"], reverse=True)
    return suggestions[:5]  # Return top 5 suggestions

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global pipeline, _cached_models, _cached_loras, _cached_vaes, _cached_upscalers, _cached_embeddings
    logger.info("ðŸ—„ï¸ Starting up - PURE DATABASE-DRIVEN MODE (no hardcoded NSFW references)")
    
    # ðŸ—„ï¸ Pre-populate GLOBAL caches for pure database-driven sync functions
    try:
        logger.info("ðŸ“Š Pre-loading samplers cache...")
        await sampler_manager.get_available_samplers()
        logger.info("ðŸ“Š Samplers cache populated successfully")
    except Exception as e:
        logger.warning(f"Could not populate samplers cache: {e}")
    
    try:
        logger.info("ðŸ—„ï¸ Pre-loading MODELS cache...")
        _cached_models = await model_manager.get_available_models()
        logger.info(f"ðŸ—„ï¸ Models cache populated: {len(_cached_models)} models")
    except Exception as e:
        logger.warning(f"Could not populate models cache: {e}")
        _cached_models = []
    
    try:
        logger.info("ðŸŽ¨ Pre-loading LORAS cache...")
        _cached_loras = await lora_manager.get_available_loras()
        logger.info(f"ðŸŽ¨ LoRAs cache populated: {len(_cached_loras)} LoRAs")
    except Exception as e:
        logger.warning(f"Could not populate LoRAs cache: {e}")
        _cached_loras = []
    
    try:
        logger.info("ðŸŽ­ Pre-loading VAES cache...")
        _cached_vaes = await vae_manager.get_available_vaes()
        logger.info(f"ðŸŽ­ VAEs cache populated: {len(_cached_vaes)} VAEs")
    except Exception as e:
        logger.warning(f"Could not populate VAEs cache: {e}")
        _cached_vaes = []
    
    try:
        logger.info("ðŸ” Pre-loading UPSCALERS cache...")
        _cached_upscalers = await upscaler_manager.get_available_upscalers()
        logger.info(f"ðŸ” Upscalers cache populated: {len(_cached_upscalers)} upscalers")
    except Exception as e:
        logger.warning(f"Could not populate upscalers cache: {e}")
        _cached_upscalers = []
    
    try:
        logger.info("ðŸ“š Pre-loading EMBEDDINGS cache...")
        _cached_embeddings = await embedding_manager.get_available_embeddings()
        logger.info(f"ðŸ“š Embeddings cache populated: {len(_cached_embeddings)} embeddings")
    except Exception as e:
        logger.warning(f"Could not populate embeddings cache: {e}")
        _cached_embeddings = []
    
    models = _cached_models
    if models:
        # ðŸ—„ï¸ DATABASE-DRIVEN: Load first available model (no hardcoded NSFW preferences)
        default_model = models[0]["path"]
        default_model_type = models[0].get("type", "sdxl")
        
        logger.info(f"ðŸ—„ï¸ Loading default model: {models[0].get('display_name', 'Unknown')}")
        success = load_model(default_model, default_model_type)
        
        # Load embeddings after model is loaded
        if success and pipeline is not None:
            logger.info("ðŸ“š Loading embeddings at startup...")
            embedding_result = await load_embeddings_to_pipeline_async(pipeline)
            if embedding_result["loaded"] > 0:
                logger.info(f"ðŸ“š Startup: Loaded {embedding_result['loaded']} embeddings")
            else:
                logger.info("ðŸ“š Startup: No embeddings found to load")
    else:
        logger.warning("No models found in models directory")
    
    yield
    
    # Shutdown
    if pipeline is not None:
        del pipeline
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    logger.info("Shutdown complete")

# FastAPI app setup
app = FastAPI(title="Image Generation API", version="2.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data Models
class GenerationRequest(BaseModel):
    prompt: str
    negative_prompt: str = ""  # Updated to match api_v2.py
    width: int = 1024  # Updated to match api_v2.py
    height: int = 1024
    num_inference_steps: int = 30  # Updated to match api_v2.py
    guidance_scale: float = 7.0  # Updated to match api_v2.py
    seed: int = -1
    sampler: str = "DPM++ 2M SDE Karras"
    model_name: str  # Added to match api_v2.py - REQUIRED for frontend compatibility
    loras: list = []  # List of {"filename": str, "scale": float} dicts
    clip_skip: Optional[int] = None  # CLIP skip parameter for text encoder (optional)
    user_id: Optional[str] = None  # Added to match api_v2.py
    # Legacy fields (optional for backward compatibility)
    force_chunking: bool = False  # For testing long prompt processing
    use_fp8: bool = False  # Real FP8 precision toggle
    auto_unload_loras: bool = True  # Automatically unload LoRAs after generation

class GenerationResponse(BaseModel):
    image: str  # base64 encoded image
    seed: int
    parameters: dict

class ModelSwitchRequest(BaseModel):
    model_name: str

class LoRARequest(BaseModel):
    lora_filename: str
    scale: float = 1.0

class LoRAListResponse(BaseModel):
    loras: list
    current_loaded: list

class EmbeddingListResponse(BaseModel):
    embeddings: list
    loaded_count: int
    categories: list

class EmbeddingSuggestionsRequest(BaseModel):
    text: str
    context: str = "prompt"  # "prompt" or "negative_prompt"

class EmbeddingLoadRequest(BaseModel):
    embedding_paths: list  # List of embedding paths to load

class TokenAnalysisRequest(BaseModel):
    prompt: str

class TokenAnalysisResponse(BaseModel):
    original_prompt: str
    original_token_count: int
    processed_prompt: str
    processed_token_count: int
    was_truncated: bool
    truncation_method: str
    excluded_text: str

# ðŸ—„ï¸ FAVORITE PROMPTS API MODELS
class CreateFavoritePromptRequest(BaseModel):
    title: str
    prompt: str
    negative_prompt: Optional[str] = None
    description: Optional[str] = None
    
    # Generation parameters
    width: int
    height: int
    steps: int
    guidance_scale: float
    seed: int
    sampler: str
    clip_skip: Optional[int] = None
    model_filename: Optional[str] = None
    loras_used: Optional[dict] = None
    
    # Organization
    tags: list = []
    category: Optional[str] = None
    is_public: bool = False
    
    # Base64 encoded image data
    image_data: str  # Base64 encoded image

class UpdateFavoritePromptRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[list] = None
    category: Optional[str] = None
    is_public: Optional[bool] = None

class FavoritePromptResponse(BaseModel):
    id: str
    title: str
    prompt: str
    negative_prompt: Optional[str]
    description: Optional[str]
    image_url: str
    thumbnail_url: Optional[str]
    
    # Generation parameters
    width: int
    height: int
    steps: int
    guidance_scale: float
    seed: int
    sampler: str
    clip_skip: Optional[int]
    model: Optional[dict]
    loras_used: Optional[dict]
    
    # Organization & Analytics
    tags: list
    category: Optional[str]
    is_public: bool
    is_featured: bool
    usage_count: int
    like_count: int
    view_count: int
    created_at: str
    updated_at: str

class FavoritePromptsListResponse(BaseModel):
    prompts: list
    total_count: int
    limit: int
    offset: int
    has_more: bool

@app.get("/")
async def root():
    """Serve the frontend"""
    try:
        with open("frontend/index.html", "r", encoding="utf-8") as f:
            content = f.read()
    except UnicodeDecodeError:
        # Fallback: read with error handling
        with open("frontend/index.html", "r", encoding="utf-8", errors="replace") as f:
            content = f.read()
    return HTMLResponse(content=content, media_type="text/html; charset=utf-8")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    
    # Check FP8 capabilities
    fp8_supported = False
    is_rtx_5090 = False
    gpu_name = None
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        is_rtx_5090 = "RTX 5090" in gpu_name or "RTX5090" in gpu_name
        
        # Test FP8 support - comprehensive check
        try:
            test_tensor = torch.randn(1, 1, device="cuda")
            test_tensor_fp8 = test_tensor.to(dtype=torch.float8_e4m3fn)
            # Test actual operations with FP8
            result = test_tensor_fp8 + test_tensor_fp8
            result = result.to(dtype=torch.float16)  # Convert back
            fp8_supported = True
        except Exception:
            fp8_supported = False
    
    return {
        "status": "healthy",
        "model_loaded": pipeline is not None,
        "current_model": current_model,
        "current_model_type": current_model_type,
        "cuda_available": torch.cuda.is_available(),
        "gpu_name": gpu_name,
        "fp8_supported": fp8_supported,
        "is_rtx_5090": is_rtx_5090,
        "triton_available": TRITON_AVAILABLE,
        "sageattention_available": SAGEATTENTION_AVAILABLE,
        "auto_fp8_enabled": False,  # No longer auto-enabled, user must explicitly request
        "fp8_recommendation": "Use 'use_fp8: true' for 2x speed boost!" if fp8_supported else "FP8 not supported on this GPU",
        "optimization_status": "Maximum RTX 5090 optimizations available!" if (TRITON_AVAILABLE and SAGEATTENTION_AVAILABLE and is_rtx_5090) else "Some optimizations missing"
    }

@app.get("/models")
async def list_models():
    """List available models"""
    models = await get_available_models_async()
    return {
        "models": models,
        "current_model": current_model
    }

@app.post("/models/sync")
async def sync_models_from_filesystem():
    """Sync models from filesystem to database"""
    try:
        logger.info("ðŸ”„ Starting model sync from filesystem...")
        result = await model_manager.sync_models()
        
        # Refresh the cached models after sync
        global _cached_models
        _cached_models = await model_manager.get_available_models()
        
        logger.info(f"âœ… Model sync complete: {result}")
        return {
            "success": True,
            "message": f"Synced {result['synced']} new models",
            "details": result
        }
    except Exception as e:
        logger.error(f"Failed to sync models: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/{model_filename}/config")
async def get_model_config(model_filename: str):
    """Get model-specific configuration including prompt defaults"""
    try:
        from database.sqlalchemy_client import get_db_client
        from database.sqlalchemy_models import AiModel
        from sqlalchemy import select
        
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            result = await session.execute(
                select(AiModel).where(AiModel.filename == model_filename)
            )
            model = result.scalar_one_or_none()
            
            if not model:
                raise HTTPException(status_code=404, detail=f"Model '{model_filename}' not found")
            
            
            # Return model configuration using dedicated database fields
            config = {
                "filename": model.filename,
                "display_name": model.display_name,
                "default_sampler": model.default_sampler,
                "prompt_defaults": {
                    "positive_prompt": model.default_positive_prompt or "",
                    "negative_prompt": model.default_negative_prompt or "",
                    "usage_notes": model.usage_notes or "",
                    "suggested_prompts": model.suggested_prompts or [],
                    "suggested_tags": model.suggested_tags or [],
                    "suggested_negative_prompts": model.suggested_negative_prompts or [],
                    "suggested_negative_tags": model.suggested_negative_tags or []
                }
            }
            
            logger.info(f"ðŸ“ Using dedicated fields - positive: {model.default_positive_prompt}, negative: {model.default_negative_prompt}, notes: {model.usage_notes}")
            
            logger.info(f"ðŸ“¦ Final config being returned: {config}")
            return config
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get model config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/{model_filename}/debug")
async def debug_model_config(model_filename: str):
    """Debug endpoint to see raw database data"""
    try:
        from database.sqlalchemy_client import get_db_client
        from database.sqlalchemy_models import AiModel
        from sqlalchemy import select
        
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            result = await session.execute(
                select(AiModel).where(AiModel.filename == model_filename)
            )
            model = result.scalar_one_or_none()
            
            if not model:
                raise HTTPException(status_code=404, detail=f"Model '{model_filename}' not found")
            
            return {
                "filename": model.filename,
                "dedicated_fields": {
                    "default_positive_prompt": model.default_positive_prompt,
                    "default_negative_prompt": model.default_negative_prompt,
                    "usage_notes": model.usage_notes,
                    "suggested_prompts": model.suggested_prompts,
                    "suggested_tags": model.suggested_tags,
                    "suggested_negative_prompts": model.suggested_negative_prompts,
                    "suggested_negative_tags": model.suggested_negative_tags
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to debug model config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

class ModelConfigRequest(BaseModel):
    prompt_defaults: Optional[Dict[str, Any]] = None

@app.put("/models/{model_filename}/config")
async def update_model_config(model_filename: str, request: ModelConfigRequest):
    """Update model-specific configuration"""
    logger.info(f"ðŸ”„ PUT request received for model config: '{model_filename}'")
    logger.info(f"ðŸ“ Request data: {request}")
    if request.prompt_defaults:
        logger.info(f"ðŸ“ Prompt defaults: {request.prompt_defaults}")
    
    try:
        from database.sqlalchemy_client import get_db_client
        from database.sqlalchemy_models import AiModel
        from sqlalchemy import select
        
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            result = await session.execute(
                select(AiModel).where(AiModel.filename == model_filename)
            )
            model = result.scalar_one_or_none()
            
            if not model:
                raise HTTPException(status_code=404, detail=f"Model '{model_filename}' not found")
            
            # Update dedicated database fields for prompt defaults
            if request.prompt_defaults is not None:
                logger.info(f"ðŸ” request.prompt_defaults received: {request.prompt_defaults}")
                
                # Save to dedicated database columns
                model.default_positive_prompt = request.prompt_defaults.get("positive_prompt", "")
                model.default_negative_prompt = request.prompt_defaults.get("negative_prompt", "")
                model.usage_notes = request.prompt_defaults.get("usage_notes", "")
                model.suggested_prompts = request.prompt_defaults.get("suggested_prompts", [])
                model.suggested_tags = request.prompt_defaults.get("suggested_tags", [])
                model.suggested_negative_prompts = request.prompt_defaults.get("suggested_negative_prompts", [])
                model.suggested_negative_tags = request.prompt_defaults.get("suggested_negative_tags", [])
                
                logger.info(f"âœ… Saved to dedicated fields:")
                logger.info(f"   - positive_prompt: {repr(model.default_positive_prompt)}")
                logger.info(f"   - negative_prompt: {repr(model.default_negative_prompt)}")
                logger.info(f"   - usage_notes: {repr(model.usage_notes)}")
                logger.info(f"   - suggested_prompts: {model.suggested_prompts}")
                logger.info(f"   - suggested_tags: {model.suggested_tags}")
                logger.info(f"   - suggested_negative_prompts: {model.suggested_negative_prompts}")
                logger.info(f"   - suggested_negative_tags: {model.suggested_negative_tags}")
            
            await session.commit()
            logger.info(f"âœ… Successfully committed changes to database")
            
            return {
                "success": True,
                "message": f"Updated configuration for {model.display_name}"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update model config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/samplers")
async def list_samplers():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: List available samplers from database
    REPLACES: Hardcoded SAMPLERS dict
    """
    try:
        samplers = await sampler_manager.get_available_samplers()
        sampler_names = [sampler['name'] for sampler in samplers]
        return {
            "samplers": sampler_names,
            "default": "DPM++ 2M SDE Karras",
            "total_count": len(sampler_names)
        }
    except Exception as e:
        logger.warning(f"Database sampler lookup failed: {e}")
        # Fallback to basic sampler list
        return {
            "samplers": ["DPM++ 2M SDE Karras", "DPM++ 2M", "Euler a", "Euler", "DDIM"],
            "default": "DPM++ 2M SDE Karras",
            "total_count": 5
        }

@app.get("/compatibility")
async def get_compatibility_info():
    """Get base model compatibility information for models and LoRAs"""
    models = await get_available_models_async()
    loras = await get_available_loras_async(filter_by_current_model=False)
    
    # Group models by base type
    models_by_base = {}
    for model in models:
        base = model["base_model"]
        if base not in models_by_base:
            models_by_base[base] = []
        models_by_base[base].append(model["display_name"])
    
    # Group LoRAs by compatibility
    loras_by_base = {}
    for lora in loras:
        for base in lora["compatible_bases"]:
            if base not in loras_by_base:
                loras_by_base[base] = []
            loras_by_base[base].append(lora["display_name"])
    
    # Current model info
    current_info = None
    if current_model:
        current_base = get_model_base_classification(current_model)
        compatible_loras = [l["display_name"] for l in loras if current_base in l["compatible_bases"]]
        current_info = {
            "model": current_model,
            "base_type": current_base,
            "compatible_loras": compatible_loras,
            "compatible_lora_count": len(compatible_loras)
        }
    
    return {
        "current_model": current_info,
        "models_by_base": models_by_base,
        "loras_by_base": loras_by_base,
        "base_types": {
            "noobai": "NoobAI models - specialized variants",
            "ponyxl": "PonyXL models - specialized variants", 
            "illustrious": "Illustrious models - specialized variants",
            "sdxl_base": "Base SDXL models - general compatibility"
        },
        "info": "ðŸ—„ï¸ Model compatibility now database-driven - no hardcoded references"
    }

@app.get("/fp8-status")
async def fp8_status():
    """Get detailed FP8 capabilities and status"""
    
    if not torch.cuda.is_available():
        return {
            "fp8_supported": False,
            "reason": "CUDA not available",
            "recommendation": "Install CUDA-enabled PyTorch for GPU acceleration"
        }
    
    gpu_name = torch.cuda.get_device_name()
    is_rtx_5090 = "RTX 5090" in gpu_name or "RTX5090" in gpu_name
    is_rtx_4090 = "RTX 4090" in gpu_name or "RTX4090" in gpu_name
    is_ampere_or_newer = any(arch in gpu_name.upper() for arch in ["RTX 30", "RTX 40", "RTX 50", "A100", "H100"])
    
    # Test FP8 support - comprehensive check
    fp8_supported = False
    fp8_error = None
    try:
        test_tensor = torch.randn(1, 1, device="cuda")
        test_tensor_fp8 = test_tensor.to(dtype=torch.float8_e4m3fn)
        # Test actual operations with FP8
        result = test_tensor_fp8 + test_tensor_fp8
        result = result.to(dtype=torch.float16)  # Convert back
        fp8_supported = True
    except Exception as e:
        fp8_supported = False
        fp8_error = str(e)
    
    # Determine performance benefit
    performance_benefit = "2x speed boost expected"
    if is_rtx_5090:
        performance_benefit = "Up to 2x speed boost - optimal RTX 5090 performance"
    elif is_rtx_4090:
        performance_benefit = "Up to 1.5x speed boost on RTX 4090"
    elif is_ampere_or_newer:
        performance_benefit = "Speed boost possible on newer GPUs"
    else:
        performance_benefit = "Limited benefit on older GPUs"
    
    return {
        "gpu_name": gpu_name,
        "fp8_supported": fp8_supported,
        "fp8_error": fp8_error,
        "is_rtx_5090": is_rtx_5090,
        "is_rtx_4090": is_rtx_4090,
        "is_ampere_or_newer": is_ampere_or_newer,
        "auto_fp8_enabled": False,  # No longer auto-enabled, user must explicitly request
        "performance_benefit": performance_benefit,
        "recommendation": {
            "text_generation": "Set 'use_fp8: true' for maximum speed" if fp8_supported else "FP8 not supported",
            "model_compatibility": {
                "sdxl_models": "Full FP8 support"
            }
        }
    }

@app.get("/loras")
async def list_loras(filter_compatible: bool = False):
    """List available LoRA models and currently loaded ones"""
    loras = await get_available_loras_async(filter_by_current_model=filter_compatible)
    
    # TODO: Track currently loaded LoRAs (would need global state management)
    current_loaded = []  # Placeholder for now
    
    # Get current model info for context
    current_model_info = None
    if current_model:
        current_base = get_model_base_classification(current_model)
        current_model_info = {
            "filename": current_model,
            "base_model": current_base
        }
    
    return {
        "loras": loras,
        "current_loaded": current_loaded,
        "current_model": current_model_info,
        "filter_compatible": filter_compatible,
        "total_count": len(loras),
        "compatibility_info": {
            "noobai_loras": len([l for l in loras if "noobai" in l["compatible_bases"]]),
            "ponyxl_loras": len([l for l in loras if "ponyxl" in l["compatible_bases"]]),
            "illustrious_loras": len([l for l in loras if "illustrious" in l["compatible_bases"]]),
            "universal_loras": len([l for l in loras if len(l["compatible_bases"]) > 1])
        }
    }

@app.post("/load-lora")
async def load_lora(request: LoRARequest):
    """Load a LoRA into the current pipeline"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    # Find the LoRA file (don't filter by compatibility here to allow explicit loading)
    loras = await get_available_loras_async(filter_by_current_model=False)
    lora_info = None
    for lora in loras:
        if lora["filename"] == request.lora_filename:
            lora_info = lora
            break
    
    if not lora_info:
        raise HTTPException(status_code=404, detail="LoRA not found")
    
    # Load single LoRA using multi-LoRA function
    loras_to_load = [{"path": lora_info["path"], "scale": request.scale, "name": lora_info["display_name"]}]
    success = load_multiple_loras_to_pipeline(pipeline, loras_to_load)
    
    if success:
        return {
            "status": "success",
            "message": f"LoRA '{lora_info['display_name']}' loaded with scale {request.scale}",
            "lora": lora_info
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to load LoRA")

@app.post("/unload-loras")
async def unload_loras():
    """Unload all LoRAs from the current pipeline"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    success = unload_loras_from_pipeline(pipeline)
    
    if success:
        return {
            "status": "success",
            "message": "All LoRAs unloaded successfully"
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to unload LoRAs")

@app.get("/embeddings", response_model=EmbeddingListResponse)
async def list_embeddings():
    """List available embeddings and their categories"""
    embeddings = await get_available_embeddings_async()
    
    # Get unique categories
    categories = list(set(emb["category"] for emb in embeddings))
    categories.sort()
    
    # Count loaded embeddings (this would need tracking in a real implementation)
    loaded_count = 0  # Placeholder - would track loaded embeddings
    
    return EmbeddingListResponse(
        embeddings=embeddings,
        loaded_count=loaded_count,
        categories=categories
    )

@app.post("/load-embeddings")
async def load_embeddings(request: EmbeddingLoadRequest):
    """Load specified embeddings into the pipeline"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    result = load_embeddings_to_pipeline(pipeline, request.embedding_paths)
    
    if result["status"] == "success":
        return {
            "status": "success",
            "message": f"Loaded {result['loaded']} out of {len(request.embedding_paths)} embeddings",
            "loaded_embeddings": result["embeddings"],
            "failed": result.get("failed", 0)
        }
    else:
        raise HTTPException(status_code=500, detail=f"Failed to load embeddings: {result.get('message', 'Unknown error')}")

@app.post("/reload-all-embeddings")
async def reload_all_embeddings():
    """Reload all available embeddings into the pipeline"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    result = load_embeddings_to_pipeline(pipeline, embeddings_to_load=None)
    
    if result["status"] == "success":
        return {
            "status": "success",
            "message": f"Loaded {result['loaded']} out of {result['available']} available embeddings",
            "loaded_embeddings": result["embeddings"],
            "failed": result.get("failed", 0)
        }
    else:
        raise HTTPException(status_code=500, detail=f"Failed to reload embeddings: {result.get('message', 'Unknown error')}")

@app.post("/embedding-suggestions")
async def get_embedding_suggestions(request: EmbeddingSuggestionsRequest):
    """Get embedding suggestions for a given text"""
    suggestions = get_embedding_suggestions_for_text(request.text)
    
    return {
        "text": request.text,
        "context": request.context,
        "suggestions": suggestions,
        "count": len(suggestions)
    }

def analyze_prompt_tokens(prompt: str, max_tokens: int = 500):
    """Analyze prompt and return detailed token information - REAL LONG PROMPT SUPPORT!"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    # SDXL models - get token counts from both tokenizers
    tokens_1 = pipeline.tokenizer.encode(prompt, add_special_tokens=False)
    tokens_2 = pipeline.tokenizer_2.encode(prompt, add_special_tokens=False)
    
    # DETAILED TOKEN ANALYSIS
    tokens_1_with_special = pipeline.tokenizer.encode(prompt, add_special_tokens=True)
    tokens_2_with_special = pipeline.tokenizer_2.encode(prompt, add_special_tokens=True)
    
    logger.info(f"ðŸ” DETAILED TOKEN ANALYSIS:")
    logger.info(f"   ðŸ“ Original prompt length: {len(prompt)} characters")
    logger.info(f"   ðŸŽ¯ Tokenizer 1 (no special): {len(tokens_1)} tokens")
    logger.info(f"   ðŸŽ¯ Tokenizer 2 (no special): {len(tokens_2)} tokens") 
    logger.info(f"   ðŸŽ¯ Tokenizer 1 (with special): {len(tokens_1_with_special)} tokens")
    logger.info(f"   ðŸŽ¯ Tokenizer 2 (with special): {len(tokens_2_with_special)} tokens")
    
    # Use the higher token count (more conservative estimate)
    original_token_count = max(len(tokens_1), len(tokens_2))
    
    # Show actual tokens for debugging
    logger.info(f"   ðŸ“œ Last 10 tokens from tokenizer 1: {tokens_1[-10:]}")
    decoded_last_tokens = pipeline.tokenizer.decode(tokens_1[-10:], skip_special_tokens=True)
    logger.info(f"   ðŸ“œ Last 10 tokens decoded: '{decoded_last_tokens}'")
    
    # Check chunking threshold
    CHUNK_THRESHOLD = 77
    would_chunk = original_token_count > CHUNK_THRESHOLD
    
    logger.info(f"ðŸŽ¯ TOKEN ANALYSIS: {original_token_count} tokens, threshold: {CHUNK_THRESHOLD}, would_chunk: {would_chunk}")
    
    # TRUE EMBEDDINGS APPROACH - NO LIMITS!
    if not would_chunk:
        # Short prompt - no chunking needed
        return TokenAnalysisResponse(
            original_prompt=prompt,
            original_token_count=original_token_count,
            processed_prompt=prompt,
            processed_token_count=original_token_count,
            was_truncated=False,
            truncation_method="standard_processing",
            excluded_text=""
        )
    else:
        # Long prompt - ALL tokens processed with TRUE embeddings approach!
        chunk_size = 77  # Each chunk is 77 tokens (max_length)
        num_chunks = (original_token_count + chunk_size - 1) // chunk_size
        
        logger.info(f"ðŸ“¦ TRUE EMBEDDINGS ANALYSIS: {num_chunks} chunks of {chunk_size} tokens each")
        
        return TokenAnalysisResponse(
            original_prompt=prompt,
            original_token_count=original_token_count,
            processed_prompt=prompt,  # Full prompt is processed!
            processed_token_count=original_token_count,  # ALL tokens used - no truncation ever!
            was_truncated=False,  # ZERO truncation - unlimited token support!
            truncation_method=f"true_embeddings_concatenation_{num_chunks}_chunks",
            excluded_text=""  # NOTHING excluded - every single token is processed!
        )

@app.post("/analyze-tokens", response_model=TokenAnalysisResponse)
async def analyze_tokens(request: TokenAnalysisRequest):
    """Analyze prompt tokens and truncation behavior"""
    return analyze_prompt_tokens(request.prompt)

@app.post("/switch-model")
async def switch_model(request: ModelSwitchRequest):
    """Switch to a different model"""
    models = await get_available_models_async()
    model_info = None
    
    for model in models:
        if model["filename"] == request.model_name:
            model_info = model
            break
    
    if not model_info:
        raise HTTPException(status_code=404, detail="Model not found")
    
    # Load model without FP8 by default (user can enable via generation request)
    success = load_model(model_info["path"], model_info["type"], use_fp8=False)
    
    if success:
        return {
            "status": "success",
            "current_model": current_model,
            "current_model_type": current_model_type,
            "message": f"Successfully switched to {model_info['display_name']}"
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to load model")

@app.post("/generate", response_model=GenerationResponse)
async def generate_image(request: GenerationRequest):
    """Generate an image based on the prompt"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    try:
        # Set scheduler based on sampler choice - SDXL models
        if request.sampler != "DPM++ 2M SDE Karras":  # Only change if different from default
            scheduler = get_scheduler(request.sampler, pipeline.scheduler.config)
            pipeline.scheduler = scheduler
            logger.info(f"Using sampler: {request.sampler}")
        
        # Set seed if provided
        generator = None
        actual_seed = request.seed
        if request.seed == -1:
            actual_seed = int(torch.randint(0, 2**32, (1,)).item())
        
        generator = torch.Generator(device="cuda" if torch.cuda.is_available() else "cpu")
        generator.manual_seed(int(actual_seed))
        
        logger.info(f"ðŸŽ¨ Generating image with prompt: {request.prompt[:50]}...")
        logger.info(f"Using sampler: {request.sampler}, Steps: {request.num_inference_steps}, Guidance: {request.guidance_scale}, CLIP Skip: {request.clip_skip or 'Default'}")
        
        # Handle LoRA loading
        loaded_loras = []
        lora_processing_info = "No LoRAs used"
        
        if request.loras and len(request.loras) > 0:
            logger.info(f"ðŸŽ¨ Processing {len(request.loras)} LoRA(s) for this generation")
            
            # Unload any existing LoRAs first for clean state
            unload_loras_from_pipeline(pipeline)
            
            # Prepare LoRAs for batch loading
            available_loras = await get_available_loras_async()
            lora_dict = {lora["filename"]: lora for lora in available_loras}
            
            loras_to_load = []
            
            for lora_request in request.loras:
                lora_filename = lora_request.get("filename", "")
                lora_scale = lora_request.get("scale", 1.0)
                
                if lora_filename in lora_dict:
                    lora_info = lora_dict[lora_filename]
                    loras_to_load.append({
                        "path": lora_info["path"],
                        "scale": lora_scale,
                        "name": lora_info["display_name"]
                    })
                    loaded_loras.append({
                        "name": lora_info["display_name"], 
                        "scale": lora_scale,
                        "filename": lora_filename
                    })
                else:
                    logger.warning(f"   âš ï¸ LoRA not found: {lora_filename}")
            
            # Load all LoRAs together with proper multi-LoRA support
            if loras_to_load:
                success = load_multiple_loras_to_pipeline(pipeline, loras_to_load)
                if success:
                    logger.info(f"   âœ… Successfully loaded {len(loras_to_load)} LoRA(s) with proper scaling")
                else:
                    logger.warning(f"   âš ï¸ Failed to load some LoRAs")
                    loaded_loras = []  # Clear if loading failed
            
            if loaded_loras:
                lora_names = [f"{lora['name']}({lora['scale']})" for lora in loaded_loras]
                lora_processing_info = f"LoRAs applied: {', '.join(lora_names)}"
                logger.info(f"ðŸŽ¨ {len(loaded_loras)} LoRA(s) successfully loaded for generation")
        
        # Get token count for logging - SDXL models
        try:
            prompt_tokens = len(pipeline.tokenizer.encode(request.prompt))
        except Exception as e:
            logger.warning(f"âš ï¸ Could not get token count: {e}, estimating instead")
            prompt_tokens = len(request.prompt) // 4
            
        logger.info(f"ðŸš€ Processing ~{prompt_tokens} tokens (FULL PROMPT SUPPORT - NO LIMITS!)")
        
        # SAFE OPTIMIZATION - no aggressive FP8 conversion to avoid dtype issues
        gpu_name = torch.cuda.get_device_name().lower() if torch.cuda.is_available() else ""
        is_rtx_5090 = "rtx 5090" in gpu_name or "rtx5090" in gpu_name
        is_rtx_4090 = "rtx 4090" in gpu_name or "rtx4090" in gpu_name
        is_modern_gpu = is_rtx_5090 or is_rtx_4090 or any(arch in gpu_name for arch in ["rtx 30", "rtx 40", "rtx 50", "a100", "h100"])
        
        # Only use FP8 if explicitly requested (no auto-enable due to dtype issues)
        auto_fp8_enabled = False  # Disabled auto-FP8 due to dtype compatibility issues
        effective_use_fp8 = request.use_fp8
        
        # Quick FP8 capability check (less conservative)
        fp8_supported = False
        if torch.cuda.is_available() and is_modern_gpu:
            try:
                # Quick test - if the dtype exists, FP8 is likely supported
                test_tensor = torch.tensor([1.0], device="cuda")
                _ = test_tensor.to(dtype=torch.float8_e4m3fn)
                fp8_supported = True
            except:
                fp8_supported = False
        
        # Only skip FP8 if explicitly unsupported, not on conservative runtime tests
        if effective_use_fp8 and not fp8_supported:
            logger.warning("âš ï¸ FP8 requested but float8_e4m3fn dtype not available - using optimized alternatives")
            effective_use_fp8 = False
        elif effective_use_fp8:
            logger.info("ðŸ”¥ FP8 PRECISION ACTIVE - safe optimization mode!")
        elif is_rtx_5090:
            logger.info("ðŸ’¡ RTX 5090 detected - set 'use_fp8: true' for additional optimizations")
        
        # LOG THE COMPLETE PROMPT for verification
        logger.info(f"ðŸ“ COMPLETE PROMPT BEING PROCESSED:")
        logger.info(f"   >>> {request.prompt}")
        logger.info(f"ðŸ“ COMPLETE NEGATIVE PROMPT:")
        logger.info(f"   >>> {request.negative_prompt}")
        
        # Handle SDXL generation
        device = "cuda" if torch.cuda.is_available() else "cpu"
        fp8_applied = False  # Track FP8 application
        
        # SDXL/Flux models - use chunked encoding for long prompts
        CHUNK_THRESHOLD = 75  # Leave room for BOS/EOS tokens
        should_chunk = prompt_tokens > CHUNK_THRESHOLD or request.force_chunking
        
        if prompt_tokens > CHUNK_THRESHOLD:
            logger.info(f"ðŸ”¥ Long prompt detected ({prompt_tokens} tokens) - using chunked processing with {((prompt_tokens + 74) // 75)} chunks")
        
        # Apply SAFE FP8 optimization to SDXL models if enabled
        if effective_use_fp8:
            try:
                # CONSERVATIVE FP8: Use compilation instead of weight conversion
                logger.info("ðŸ” Attempting conservative SDXL FP8 optimization...")
                
                # Use torch.compile for optimization instead of weight conversion
                if hasattr(torch, 'compile') and hasattr(pipeline, 'unet'):
                    try:
                        pipeline.unet = torch.compile(pipeline.unet, mode="max-autotune")
                        logger.info("âœ… SDXL UNet compiled with max optimization")
                        fp8_applied = True
                    except Exception as e:
                        logger.warning(f"Could not compile UNet: {e}")
                
                # Enable RTX 5090 optimizations
                if is_rtx_5090:
                    try:
                        torch.backends.cuda.matmul.allow_tf32 = True
                        torch.backends.cudnn.allow_tf32 = True
                        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
                        logger.info("âœ… SDXL RTX 5090 optimizations enabled (TF32, FP16 reduction)")
                        fp8_applied = True
                    except Exception as e:
                        logger.warning(f"Could not enable RTX 5090 optimizations: {e}")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ Could not enable FP8 optimizations for SDXL: {e}, using standard precision")
                fp8_applied = False
        else:
            # Apply standard optimizations for non-FP8 mode
            if is_rtx_5090:
                try:
                    torch.backends.cuda.matmul.allow_tf32 = True
                    torch.backends.cudnn.allow_tf32 = True
                    logger.info("âœ… Standard SDXL RTX 5090 optimizations enabled (TF32)")
                except Exception as e:
                    logger.warning(f"Could not enable standard optimizations: {e}")
        
        logger.info(f"ðŸŽ¯ SDXL CHUNK DECISION:")
        logger.info(f"   ðŸ“Š Tokens: {prompt_tokens}")
        logger.info(f"   ðŸ“ Threshold: {CHUNK_THRESHOLD}")
        logger.info(f"   ðŸ”§ Force chunking: {request.force_chunking}")
        logger.info(f"   âœ… Will use chunking: {should_chunk}")
        
        if should_chunk:
            logger.info(f"ðŸ”¥ CHUNKED MODE ACTIVATED! Using advanced long prompt encoding for {prompt_tokens} tokens")
            
            # Use chunked encoding for long prompts
            try:
                logger.info(f"ðŸ”§ Attempting chunked processing for prompt with {prompt_tokens} tokens...")
                
                # Process prompts with TRUE embeddings approach - no 77-token limit!
                prompt_embeds_1, prompt_embeds_2, pooled_prompt_embeds, pos_chunk_count = encode_long_prompt_with_embeddings(
                    pipeline, request.prompt, device=device, clip_skip=request.clip_skip
                )
                logger.info(f"   âœ… Positive prompt processed with unlimited tokens!")
                
                negative_prompt_embeds_1, negative_prompt_embeds_2, negative_pooled_prompt_embeds, neg_chunk_count = encode_long_prompt_with_embeddings(
                    pipeline, request.negative_prompt, device=device, clip_skip=request.clip_skip
                )
                logger.info(f"   âœ… Negative prompt processed with unlimited tokens!")
                
                # CRITICAL: SDXL requires BOTH text encoders concatenated along CHANNEL dimension
                # Based on research: (1,77,768) + (1,77,1280) = (1,77,2048)
                logger.info(f"   ðŸ”— Concatenating SDXL dual-encoder embeddings along CHANNEL dimension:")
                logger.info(f"      prompt_embeds_1: {prompt_embeds_1.shape} (CLIP ViT-L)")
                logger.info(f"      prompt_embeds_2: {prompt_embeds_2.shape} (OpenCLIP ViT-bigG)")
                logger.info(f"      negative_prompt_embeds_1: {negative_prompt_embeds_1.shape}")
                logger.info(f"      negative_prompt_embeds_2: {negative_prompt_embeds_2.shape}")
                
                # SDXL: Concatenate along CHANNEL dimension (dim=2)
                prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=2)
                negative_prompt_embeds = torch.cat([negative_prompt_embeds_1, negative_prompt_embeds_2], dim=2)
                
                logger.info(f"   âœ… SDXL channel concatenation complete:")
                logger.info(f"      Final prompt_embeds: {prompt_embeds.shape} (should be [1,77,2048])")
                logger.info(f"      Final negative_prompt_embeds: {negative_prompt_embeds.shape}")
                
                # CRITICAL: Ensure both embeddings have the same sequence length
                if prompt_embeds.shape[1] != negative_prompt_embeds.shape[1]:
                    logger.warning(f"Sequence length mismatch: positive={prompt_embeds.shape[1]}, negative={negative_prompt_embeds.shape[1]}")
                    # Pad the shorter one to match the longer one
                    max_len = max(prompt_embeds.shape[1], negative_prompt_embeds.shape[1])
                    
                    if prompt_embeds.shape[1] < max_len:
                        pad_size = max_len - prompt_embeds.shape[1]
                        padding = torch.zeros(1, pad_size, prompt_embeds.shape[2], device=prompt_embeds.device, dtype=prompt_embeds.dtype)
                        prompt_embeds = torch.cat([prompt_embeds, padding], dim=1)
                        logger.info(f"   ðŸ”§ Padded positive embeddings to {prompt_embeds.shape}")
                    
                    if negative_prompt_embeds.shape[1] < max_len:
                        pad_size = max_len - negative_prompt_embeds.shape[1]
                        padding = torch.zeros(1, pad_size, negative_prompt_embeds.shape[2], device=negative_prompt_embeds.device, dtype=negative_prompt_embeds.dtype)
                        negative_prompt_embeds = torch.cat([negative_prompt_embeds, padding], dim=1)
                        logger.info(f"   ðŸ”§ Padded negative embeddings to {negative_prompt_embeds.shape}")
                
                logger.info(f"   âœ… Final embeddings ready:")
                logger.info(f"      prompt_embeds: {prompt_embeds.shape}")
                logger.info(f"      negative_prompt_embeds: {negative_prompt_embeds.shape}")
                logger.info(f"      pooled_prompt_embeds: {pooled_prompt_embeds.shape if pooled_prompt_embeds is not None else 'None'}")
                logger.info(f"      negative_pooled_prompt_embeds: {negative_pooled_prompt_embeds.shape if negative_pooled_prompt_embeds is not None else 'None'}")
                
                # Validate final embeddings
                if prompt_embeds is None or negative_prompt_embeds is None:
                    raise RuntimeError("Failed to get valid concatenated embeddings")
                
                # Generate with TRUE embeddings - unlimited prompt length!
                logger.info(f"   ðŸš€ Starting SDXL generation with TRUE embeddings (no token limits)...")
                
                # Ensure embeddings are properly formatted for SDXL
                prompt_embeds = prompt_embeds.to(device)
                negative_prompt_embeds = negative_prompt_embeds.to(device)
                logger.info(f"   âœ… Final embeddings ready for generation:")
                logger.info(f"      prompt_embeds: {prompt_embeds.shape}")
                logger.info(f"      negative_prompt_embeds: {negative_prompt_embeds.shape}")
                
                # CRITICAL: SDXL requires pooled embeddings to be provided with prompt_embeds
                if pooled_prompt_embeds is None:
                    logger.error("âŒ pooled_prompt_embeds is None - SDXL requires valid pooled embeddings!")
                    raise RuntimeError("SDXL requires pooled_prompt_embeds when prompt_embeds are provided")
                
                pooled_prompt_embeds = pooled_prompt_embeds.to(device)
                if len(pooled_prompt_embeds.shape) != 2 or pooled_prompt_embeds.shape[0] != 1:
                    logger.warning(f"Fixing pooled_prompt_embeds shape: {pooled_prompt_embeds.shape}")
                    if len(pooled_prompt_embeds.shape) == 1:
                        pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)
                    elif pooled_prompt_embeds.shape[0] != 1:
                        pooled_prompt_embeds = pooled_prompt_embeds[:1]
                
                if negative_pooled_prompt_embeds is None:
                    logger.error("âŒ negative_pooled_prompt_embeds is None - SDXL requires valid negative pooled embeddings!")
                    raise RuntimeError("SDXL requires negative_pooled_prompt_embeds when negative_prompt_embeds are provided")
                
                negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.to(device)
                if len(negative_pooled_prompt_embeds.shape) != 2 or negative_pooled_prompt_embeds.shape[0] != 1:
                    logger.warning(f"Fixing negative_pooled_prompt_embeds shape: {negative_pooled_prompt_embeds.shape}")
                    if len(negative_pooled_prompt_embeds.shape) == 1:
                        negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.unsqueeze(0)
                    elif negative_pooled_prompt_embeds.shape[0] != 1:
                        negative_pooled_prompt_embeds = negative_pooled_prompt_embeds[:1]
                
                logger.info(f"   âœ… All SDXL embeddings validated - pooled shapes: pos={pooled_prompt_embeds.shape}, neg={negative_pooled_prompt_embeds.shape}")
                
                with torch.inference_mode():
                    # DEBUG: Log exact shapes being passed to pipeline
                    logger.info(f"   ðŸ› DEBUG - Pipeline inputs:")
                    logger.info(f"      prompt_embeds: {prompt_embeds.shape if prompt_embeds is not None else 'None'}")
                    logger.info(f"      negative_prompt_embeds: {negative_prompt_embeds.shape if negative_prompt_embeds is not None else 'None'}")
                    logger.info(f"      pooled_prompt_embeds: {pooled_prompt_embeds.shape if pooled_prompt_embeds is not None else 'None'}")
                    logger.info(f"      negative_pooled_prompt_embeds: {negative_pooled_prompt_embeds.shape if negative_pooled_prompt_embeds is not None else 'None'}")
                     
                    # TRUE embeddings are properly concatenated from both encoders
                    logger.info(f"   ðŸ” Using TRUE embeddings with unlimited token support:")
                    logger.info(f"      prompt_embeds: {prompt_embeds.shape}")
                    logger.info(f"      negative_prompt_embeds: {negative_prompt_embeds.shape}")
                    
                    result = pipeline(
                        prompt_embeds=prompt_embeds,  # Single validated embeddings
                        negative_prompt_embeds=negative_prompt_embeds,  # Single validated negative embeddings
                        pooled_prompt_embeds=pooled_prompt_embeds,  # type: ignore
                        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,  # type: ignore
                        width=request.width,
                        height=request.height,
                        num_inference_steps=request.num_inference_steps,
                        guidance_scale=request.guidance_scale,
                        generator=generator,
                        cross_attention_kwargs={"scale": 1.0},  # PEFT integration compatibility
                        output_type="pil"
                    )
                
                logger.info(f"   âœ… SDXL generation completed successfully!")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Chunked prompt processing failed: {type(e).__name__}: {str(e)[:100]}...")
                logger.info("ðŸ”„ Falling back to standard SDXL generation (prompt will be truncated to 77 tokens)")
                
                # Fallback to standard generation if long prompt fails
                with torch.inference_mode():
                    result = pipeline(
                        prompt=request.prompt,  # Use full prompt, let SDXL handle truncation
                        negative_prompt=request.negative_prompt,
                        width=request.width,
                        height=request.height,
                        num_inference_steps=request.num_inference_steps,
                        guidance_scale=request.guidance_scale,
                        generator=generator,
                        cross_attention_kwargs={"scale": 1.0},  # PEFT integration compatibility
                        output_type="pil"
                    )
                logger.info("âœ… Fallback generation completed successfully")
        else:
            logger.info(f"âœ… Using standard SDXL generation for {prompt_tokens} tokens")
            # Use standard generation for short prompts
            with torch.inference_mode():
                result = pipeline(
                    prompt=request.prompt,
                    negative_prompt=request.negative_prompt,
                    width=request.width,
                    height=request.height,
                    num_inference_steps=request.num_inference_steps,
                    guidance_scale=request.guidance_scale,
                    generator=generator,
                    cross_attention_kwargs={"scale": 1.0},  # PEFT integration compatibility
                    output_type="pil"
                )

        
        # Convert to base64 - handle different return formats robustly
        image = None
        
        try:
            # Handle different pipeline return types
            if isinstance(result, dict):
                if 'images' in result:
                    image = result['images'][0]
                elif 'samples' in result:
                    image = result['samples'][0]
            elif hasattr(result, 'images') and getattr(result, 'images', None) is not None:
                images_attr = getattr(result, 'images')
                if isinstance(images_attr, (list, tuple)) and len(images_attr) > 0:
                    image = images_attr[0]
            elif isinstance(result, (tuple, list)):
                if len(result) > 0:
                    first_item = result[0]
                    if isinstance(first_item, Image.Image):
                        image = first_item
                    elif hasattr(first_item, 'images'):
                        images_attr = getattr(first_item, 'images', None)
                        if images_attr is not None and isinstance(images_attr, (list, tuple)) and len(images_attr) > 0:
                            image = images_attr[0]
                    elif isinstance(first_item, dict) and 'images' in first_item:
                        image = first_item['images'][0]
            
            # Final fallback - try to extract from result directly
            if image is None and isinstance(result, Image.Image):
                image = result
                
        except Exception as e:
            logger.error(f"Error extracting image from result: {e}")
            logger.error(f"Result type: {type(result)}")
            if hasattr(result, '__dict__'):
                logger.error(f"Result attributes: {list(result.__dict__.keys())}")
            
        if image is None:
            raise RuntimeError(f"Could not extract image from pipeline result. Result type: {type(result)}")
        
        buffer = io.BytesIO()
        image.save(buffer, format="PNG")
        img_str = base64.b64encode(buffer.getvalue()).decode()
        
        # Clean up LoRAs if requested
        if request.auto_unload_loras and loaded_loras:
            logger.info("ðŸ§¹ Auto-unloading LoRAs after generation")
            unload_loras_from_pipeline(pipeline)
        
        return GenerationResponse(
            image=img_str,
            seed=actual_seed,
            parameters={
                "prompt": request.prompt,  # FULL PROMPT - exactly what was processed
                "negative_prompt": request.negative_prompt,
                "width": request.width,
                "height": request.height,
                "steps": request.num_inference_steps,
                "guidance_scale": request.guidance_scale,
                "sampler": request.sampler,
                "model": current_model,
                "prompt_tokens": prompt_tokens,
                "processing_method": "sd_embed_inspired_smart_chunking" if should_chunk else "standard_processing",
                "chunks_used": max(pos_chunk_count, neg_chunk_count) if should_chunk else 1,
                "total_sequence_length": prompt_embeds.shape[1] if should_chunk and prompt_embeds is not None else 77,
                "zero_truncation": False,  # NO truncation - unlimited tokens supported!
                "chunking_forced": request.force_chunking,
                "chunk_threshold": 77,
                "long_prompt_support": "SD_EMBED-inspired approach - smart chunking at natural boundaries with full semantic preservation",
                "fp8_enabled": effective_use_fp8,
                "fp8_applied": fp8_applied,
                "auto_fp8": auto_fp8_enabled,  # Shows if auto-enabled for RTX 5090
                "fp8_runtime_supported": fp8_supported,
                # LoRA information
                "loras_used": loaded_loras,
                "lora_processing": lora_processing_info,
                "lora_auto_unload": request.auto_unload_loras,
                "clip_skip": request.clip_skip
            }
        )
        
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

async def get_available_vaes_async():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available VAEs from database (async version)
    For use within async contexts
    """
    try:
        vaes = await vae_manager.get_available_vaes()
        return vaes
    except Exception as e:
        logger.warning(f"Database VAE lookup failed: {e}")
        # Fallback to filesystem scan if database fails
        vaes = []
        if os.path.exists(VAE_DIR):
            vae_files = glob.glob(os.path.join(VAE_DIR, "*.safetensors"))
            for file_path in vae_files:
                filename = os.path.basename(file_path)
                display_name = filename.replace(".safetensors", "").replace("_", " ").replace("-", " ")
                vaes.append({
                    "filename": filename,
                    "display_name": display_name,
                    "path": file_path,
                    "type": "sdxl",
                    "source": "local"
                })
        return vaes

def get_available_vaes():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available VAEs from database (sync version)
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup to avoid async context issues
    global _cached_vaes
    try:
        if '_cached_vaes' in globals() and _cached_vaes:
            return _cached_vaes
        else:
            logger.warning("No cached VAE data available - database cache not populated yet")
            return []
    except Exception as e:
        logger.error(f"Error accessing cached VAE data: {e}")
        return []

def load_custom_vae(pipeline, vae_path):
    """
    Load a custom VAE into the pipeline
    
    Args:
        pipeline: The diffusers pipeline
        vae_path: Path to the VAE .safetensors file
    
    Returns:
        bool: Success status
    """
    try:
        logger.info(f"ðŸŽ¨ Loading custom VAE: {os.path.basename(vae_path)}")
        
        # Load VAE using diffusers - match the pipeline's loading pattern
        from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL
        
        custom_vae = AutoencoderKL.from_single_file(
            vae_path,
            torch_dtype=torch.float16,  # Match pipeline dtype
            use_safetensors=True
        )
        
        # Move to same device as pipeline (match existing working pattern)
        if torch.cuda.is_available() and pipeline.device.type == 'cuda':
            custom_vae = custom_vae.to(pipeline.device)
            logger.info(f"ðŸŽ¨ Moved VAE to device: {pipeline.device}")
        
        # Replace pipeline VAE
        pipeline.vae = custom_vae
        
        logger.info(f"âœ… Successfully loaded custom VAE: {os.path.basename(vae_path)}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to load VAE: {e}")
        return False

@app.get("/vaes")
async def list_vaes():
    """List available VAE models"""
    vaes = await get_available_vaes_async()
    
    return {
        "vaes": vaes,
        "current_vae": "default",  # Could track current VAE if needed
        "total_count": len(vaes)
    }

@app.post("/load-vae")
async def load_vae(request: dict):
    """Load a custom VAE into the current pipeline"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    vae_filename = request.get("vae_filename")
    if not vae_filename:
        raise HTTPException(status_code=400, detail="VAE filename required")
    
    # Find the VAE file
    vaes = await get_available_vaes_async()
    vae_info = None
    for vae in vaes:
        if vae["filename"] == vae_filename:
            vae_info = vae
            break
    
    if not vae_info:
        raise HTTPException(status_code=404, detail="VAE not found")
    
    # Load the VAE
    success = load_custom_vae(pipeline, vae_info["path"])
    
    if success:
        return {
            "status": "success",
            "message": f"VAE '{vae_info['display_name']}' loaded successfully",
            "vae": vae_info
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to load VAE")

@app.post("/reset-vae")
async def reset_vae():
    """Reset to default VAE (requires model reload)"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="No model loaded")
    
    # To reset VAE, we need to reload the model
    # This could be optimized by storing the original VAE
    return {
        "status": "info",
        "message": "To reset VAE, please reload the model",
        "recommendation": "Use /switch-model endpoint to reload current model"
    }

async def get_available_upscalers_async():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available upscalers from database (async version)
    For use within async contexts
    """
    try:
        upscalers = await upscaler_manager.get_available_upscalers()
        return upscalers
    except Exception as e:
        logger.warning(f"Database upscaler lookup failed: {e}")
        # Fallback to filesystem scan if database fails
        upscalers = []
        if os.path.exists(UPSCALER_DIR):
            file_patterns = ["*.pt", "*.pth", "*.safetensors"]
            for pattern in file_patterns:
                upscaler_files = glob.glob(os.path.join(UPSCALER_DIR, "**", pattern), recursive=True)
                for file_path in upscaler_files:
                    filename = os.path.basename(file_path)
                    model_name = os.path.splitext(filename)[0]
                    upscalers.append({
                        "filename": filename,
                        "model_name": model_name,
                        "path": file_path,
                        "type": "general",
                        "scale_factor": 4,
                        "source": "local"
                    })
        return upscalers

def get_available_upscalers():
    """
    ðŸ—„ï¸ DATABASE-DRIVEN: Get available upscalers from database (sync version)
    Uses cached data to avoid async context issues - PURE DATABASE APPROACH
    """
    # Use global cache populated during startup to avoid async context issues
    global _cached_upscalers
    try:
        if '_cached_upscalers' in globals() and _cached_upscalers:
            return _cached_upscalers
        else:
            logger.warning("No cached upscaler data available - database cache not populated yet")
            return []
    except Exception as e:
        logger.error(f"Error accessing cached upscaler data: {e}")
        return []

def upscale_image_with_model(image, upscaler_path, scale_factor=4):
    """
    Upscale an image using a loaded upscaler model
    
    Args:
        image: PIL Image to upscale
        upscaler_path: Path to the upscaler model file
        scale_factor: Target scale factor
    
    Returns:
        PIL Image: Upscaled image
    """
    try:
        try:
            import cv2  # type: ignore
            import numpy as np
        except ImportError:
            logger.error("âŒ OpenCV (cv2) not available - upscaling requires opencv-python")
            # Fallback to basic PIL upscaling
            new_size = (image.width * scale_factor, image.height * scale_factor)
            upscaled = image.resize(new_size, Image.Resampling.LANCZOS)
            logger.info(f"âœ… Basic PIL upscaling fallback: {image.size} â†’ {upscaled.size}")
            return upscaled
        
        logger.info(f"ðŸ” Upscaling image using: {os.path.basename(upscaler_path)}")
        logger.info(f"ðŸ” Original image size: {image.size}")
        logger.info(f"ðŸ” Target scale factor: {scale_factor}x")
        logger.info(f"ðŸ” Expected output size: {image.width * scale_factor}x{image.height * scale_factor}")
        
        # Convert PIL to OpenCV format
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # Try Real-ESRGAN first (most common)
        try:
            # Try importing Real-ESRGAN (optional dependencies)
            try:
                from basicsr.archs.rrdbnet_arch import RRDBNet  # type: ignore
                from realesrgan import RealESRGANer  # type: ignore
                logger.info("âœ… Real-ESRGAN libraries imported successfully")
            except ImportError as e:
                logger.error(f"âŒ Real-ESRGAN import failed: {e}")
                raise ImportError("Real-ESRGAN dependencies not available")
            
            # Check if upscaler file exists
            if not os.path.exists(upscaler_path):
                logger.error(f"âŒ Upscaler file not found: {upscaler_path}")
                raise FileNotFoundError(f"Upscaler file not found: {upscaler_path}")
            
            logger.info(f"âœ… Upscaler file found: {upscaler_path}")
            
            # Determine model parameters based on filename
            filename = os.path.basename(upscaler_path).lower()
            logger.info(f"ðŸ” Analyzing upscaler filename: {filename}")
            
            if "animevideov3" in filename:
                model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=scale_factor)
                logger.info("ðŸŽ¯ Using AnimeVideo v3 architecture")
            elif "anime" in filename or "waifu" in filename:
                model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=scale_factor)
                logger.info("ðŸŽ¯ Using Anime/Waifu architecture")
            else:
                # Default ESRGAN architecture
                model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=scale_factor)
                logger.info("ðŸŽ¯ Using Default ESRGAN architecture")
            
            # Create upscaler with better error handling
            logger.info("ðŸ”§ Creating RealESRGANer instance...")
            try:
                upsampler = RealESRGANer(
                    scale=scale_factor,
                    model_path=upscaler_path,
                    model=model,
                    tile=512,  # Tile size for memory efficiency
                    tile_pad=10,
                    pre_pad=0,
                    half=torch.cuda.is_available(),  # Use FP16 if CUDA available
                    device='cuda' if torch.cuda.is_available() else 'cpu'
                )
                logger.info("âœ… RealESRGANer created successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to create RealESRGANer: {e}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                raise e
            
            # Upscale the image with detailed logging
            logger.info("ðŸš€ Starting Real-ESRGAN upscaling...")
            try:
                output, _ = upsampler.enhance(cv_image, outscale=scale_factor)
                logger.info("âœ… Real-ESRGAN enhancement completed")
            except Exception as e:
                logger.error(f"âŒ Real-ESRGAN enhancement failed: {e}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                # This is probably where the 'params' error is coming from
                raise e
            
            # Convert back to PIL
            output_pil = Image.fromarray(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))
            
            logger.info(f"âœ… Real-ESRGAN upscaling successful: {image.size} â†’ {output_pil.size}")
            
            # Verify the upscaling actually worked
            if output_pil.size == image.size:
                logger.warning("âš ï¸ Output size same as input - Real-ESRGAN may have failed silently")
                raise RuntimeError("Real-ESRGAN returned same size - likely failed")
            
            return output_pil
            
        except Exception as real_esrgan_error:
            logger.warning(f"âš ï¸ Real-ESRGAN failed ({type(real_esrgan_error).__name__}: {real_esrgan_error}), falling back to basic upscaling")
            
            # Fallback to basic upscaling that actually works
            new_size = (image.width * scale_factor, image.height * scale_factor)
            upscaled = image.resize(new_size, Image.Resampling.LANCZOS)
            
            logger.info(f"âœ… Basic upscaling fallback: {image.size} â†’ {upscaled.size}")
            return upscaled
            
    except Exception as e:
        logger.error(f"âŒ All upscaling methods failed: {e}")
        logger.error(f"âŒ Error type: {type(e).__name__}")
        # Return original image if everything fails
        logger.warning("âš ï¸ Returning original image unchanged")
        return image

@app.get("/upscalers")
async def list_upscalers():
    """List available upscaler models"""
    upscalers = await get_available_upscalers_async()
    
    # Group by type for better organization
    upscalers_by_type = {}
    for upscaler in upscalers:
        upscaler_type = upscaler["type"]
        if upscaler_type not in upscalers_by_type:
            upscalers_by_type[upscaler_type] = []
        upscalers_by_type[upscaler_type].append(upscaler)
    
    return {
        "upscalers": upscalers,
        "upscalers_by_type": upscalers_by_type,
        "total_count": len(upscalers),
        "supported_formats": [".pt", ".pth", ".safetensors"],
        "categories": list(set(u["category"] for u in upscalers)),
        "scale_factors_available": sorted(list(set(u["scale_factor"] for u in upscalers)))
    }

@app.post("/upscale")
async def upscale_image(request: dict):
    """Upscale an image using a specified upscaler model"""
    
    # Debug: Log the request structure to find the 'params' issue
    logger.info(f"ðŸ” Upscale request keys: {list(request.keys())}")
    logger.info(f"ðŸ” Upscale request: {request}")
    
    # Get request parameters
    image_data = request.get("image")  # Base64 encoded image
    upscaler_filename = request.get("upscaler_filename")
    custom_scale = request.get("scale_factor")  # Optional custom scale
    
    if not image_data:
        raise HTTPException(status_code=400, detail="Image data required")
    
    if not upscaler_filename:
        raise HTTPException(status_code=400, detail="Upscaler filename required")
    
    # Find the upscaler file
    upscalers = await get_available_upscalers_async()
    upscaler_info = None
    for upscaler in upscalers:
        if upscaler["filename"] == upscaler_filename:
            upscaler_info = upscaler
            break
    
    if not upscaler_info:
        raise HTTPException(status_code=404, detail="Upscaler not found")
    
    try:
        # Decode base64 image
        import base64
        image_bytes = base64.b64decode(image_data)
        input_image = Image.open(io.BytesIO(image_bytes))
        
        # Use the model's native scale factor for Real-ESRGAN models
        # Custom scale is only used for basic/fallback upscaling
        model_scale = upscaler_info["scale_factor"]
        scale_factor = custom_scale if custom_scale else model_scale
        
        # For Real-ESRGAN models, warn if custom scale doesn't match model
        if custom_scale and custom_scale != model_scale and upscaler_info["type"] in ["real-esrgan", "esrgan"]:
            logger.warning(f"âš ï¸ Model {upscaler_info['model_name']} is trained for {model_scale}x, but {custom_scale}x requested")
            logger.warning(f"âš ï¸ Using model's native scale factor: {model_scale}x")
            scale_factor = model_scale  # Force model's native scale for Real-ESRGAN
        
        logger.info(f"ðŸ” Upscaling {input_image.size} image with {upscaler_info['model_name']} (scale: {scale_factor}x)")
        
        # Upscale the image
        upscaled_image = upscale_image_with_model(
            input_image, 
            upscaler_info["path"], 
            scale_factor
        )
        
        # Convert result to base64
        buffer = io.BytesIO()
        upscaled_image.save(buffer, format="PNG")
        result_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            "status": "success",
            "upscaled_image": result_b64,
            "original_size": {"width": input_image.width, "height": input_image.height},
            "upscaled_size": {"width": upscaled_image.width, "height": upscaled_image.height},
            "scale_factor_used": scale_factor,
            "upscaler_used": {
                "name": upscaler_info["model_name"],
                "type": upscaler_info["type"],
                "filename": upscaler_info["filename"]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Upscaling failed: {e}")
        raise HTTPException(status_code=500, detail=f"Upscaling failed: {str(e)}")

# ðŸ—„ï¸ FAVORITE PROMPTS API ENDPOINTS

@app.post("/favorite-prompts", response_model=FavoritePromptResponse)
async def create_favorite_prompt(request: CreateFavoritePromptRequest):
    """Create a new favorite prompt with image upload"""
    try:
        # Debug logging
        logger.info(f"ðŸ” Creating favorite prompt:")
        logger.info(f"   Title: {request.title}")
        logger.info(f"   Model filename: {request.model_filename}")
        logger.info(f"   Image data length: {len(request.image_data) if request.image_data else 0}")
        
        # Decode base64 image
        image_bytes = base64.b64decode(request.image_data)
        
        # Prepare prompt data
        prompt_data = {
            "title": request.title,
            "prompt": request.prompt,
            "negative_prompt": request.negative_prompt,
            "description": request.description,
            "width": request.width,
            "height": request.height,
            "steps": request.steps,
            "guidance_scale": request.guidance_scale,
            "seed": request.seed,
            "sampler": request.sampler,
            "clip_skip": request.clip_skip,
            "model_filename": request.model_filename,
            "loras_used": request.loras_used,
            "tags": request.tags,
            "category": request.category,
            "is_public": request.is_public
        }
        
        # Create favorite prompt
        result = await prompt_service.create_favorite_prompt(prompt_data, image_bytes)
        
        return FavoritePromptResponse(**result)
        
    except Exception as e:
        logger.error(f"Failed to create favorite prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/favorite-prompts", response_model=FavoritePromptsListResponse)
async def get_favorite_prompts(
    limit: int = 50,
    offset: int = 0,
    category: Optional[str] = None,
    tags: Optional[str] = None,  # Comma-separated tags
    is_public: Optional[bool] = None,
    search: Optional[str] = None
):
    """Get favorite prompts with filtering and pagination"""
    try:
        # Parse tags if provided
        tag_list = None
        if tags:
            tag_list = [tag.strip() for tag in tags.split(",")]
        
        result = await prompt_service.get_favorite_prompts(
            limit=limit,
            offset=offset,
            category=category,
            tags=tag_list,
            is_public=is_public,
            search=search
        )
        
        return FavoritePromptsListResponse(**result)
        
    except Exception as e:
        logger.error(f"Failed to get favorite prompts: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/favorite-prompts/{prompt_id}", response_model=FavoritePromptResponse)
async def get_favorite_prompt(prompt_id: str):
    """Get a specific favorite prompt by ID"""
    try:
        result = await prompt_service.get_favorite_prompt_by_id(prompt_id)
        
        if not result:
            raise HTTPException(status_code=404, detail="Favorite prompt not found")
        
        return FavoritePromptResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get favorite prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/favorite-prompts/{prompt_id}", response_model=FavoritePromptResponse)
async def update_favorite_prompt(prompt_id: str, request: UpdateFavoritePromptRequest):
    """Update a favorite prompt"""
    try:
        updates = {k: v for k, v in request.dict().items() if v is not None}
        
        result = await prompt_service.update_favorite_prompt(prompt_id, updates)
        
        if not result:
            raise HTTPException(status_code=404, detail="Favorite prompt not found")
        
        return FavoritePromptResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update favorite prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/favorite-prompts/{prompt_id}")
async def delete_favorite_prompt(prompt_id: str):
    """Delete a favorite prompt and its images"""
    try:
        success = await prompt_service.delete_favorite_prompt(prompt_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Favorite prompt not found")
        
        return {"message": "Favorite prompt deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete favorite prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/favorite-prompts/{prompt_id}/like")
async def toggle_like_prompt(prompt_id: str, like: bool = True):
    """Like or unlike a favorite prompt"""
    try:
        new_count = await prompt_service.toggle_like(prompt_id, like)
        return {
            "prompt_id": prompt_id,
            "liked": like,
            "like_count": new_count
        }
        
    except Exception as e:
        logger.error(f"Failed to toggle like: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/favorite-prompts/{prompt_id}/use")
async def mark_prompt_used(prompt_id: str):
    """Mark a prompt as used (increment usage count)"""
    try:
        success = await prompt_service.increment_usage(prompt_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Favorite prompt not found")
        
        return {"message": "Usage count incremented"}
        
    except Exception as e:
        logger.error(f"Failed to mark prompt as used: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/popular-prompts")
async def get_popular_prompts(limit: int = 20):
    """Get most popular public prompts"""
    try:
        prompts = await prompt_service.get_popular_prompts(limit=limit)
        return {"prompts": prompts}
        
    except Exception as e:
        logger.error(f"Failed to get popular prompts: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/favorite-prompts/{prompt_id}/reuse")
async def get_prompt_for_reuse(prompt_id: str):
    """Get prompt parameters for reusing in generation"""
    try:
        prompt = await prompt_service.get_favorite_prompt_by_id(prompt_id)
        
        if not prompt:
            raise HTTPException(status_code=404, detail="Favorite prompt not found")
        
        # Mark as used
        await prompt_service.increment_usage(prompt_id)
        
        # Return generation parameters
        return {
            "prompt": prompt["prompt"],
            "negative_prompt": prompt["negative_prompt"],
            "width": prompt["width"],
            "height": prompt["height"],
            "steps": prompt["steps"],
            "guidance_scale": prompt["guidance_scale"],
            "seed": prompt["seed"],
            "sampler": prompt["sampler"],
            "clip_skip": prompt["clip_skip"],
            "loras_used": prompt["loras_used"],
            "model": prompt["model"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get prompt for reuse: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/loras")
async def create_lora(lora_data: dict):
    """Create a new LoRA entry in the database"""
    try:
        db_client = await get_db_client()
        lora_service = LoraService(db_client)
        
        # Extract and format data for LoRA creation
        formatted_data = {
            "filename": lora_data["filename"],
            "display_name": lora_data.get("display_name", lora_data["filename"].replace('.safetensors', '')),
            "description": lora_data.get("description"),
            "category": lora_data.get("category", "GENERAL"),
            "is_nsfw": lora_data.get("is_nsfw", False),
            "is_active": True,
            "default_scale": lora_data.get("default_scale", 1.0),
            "min_scale": lora_data.get("min_scale", -5.0),
            "max_scale": lora_data.get("max_scale", 5.0),
            "recommended_min": lora_data.get("recommended_min", 0.5),
            "recommended_max": lora_data.get("recommended_max", 1.5),
            "trigger_words": lora_data.get("trigger_words", []),
            "usage_tips": lora_data.get("usage_tips"),
            "author": lora_data.get("author"),
            "version": lora_data.get("version"),
            "website": lora_data.get("website"),
            "source": "local",
            "tags": lora_data.get("tags", []),
            "file_path": f"loras/{lora_data['filename']}"
        }
        
        lora = await lora_service.create_lora(formatted_data)
        
        return {
            "status": "success",
            "message": f"LoRA '{lora.display_name}' created successfully",
            "lora": {
                "id": lora.id,
                "filename": lora.filename,
                "display_name": lora.display_name,
                "category": lora.category.value if hasattr(lora.category, 'value') else str(lora.category)
            }
        }
        
    except Exception as e:
        logger.error(f"Error creating LoRA: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/loras/{lora_filename}")
async def update_lora(lora_filename: str, lora_data: dict):
    """Update an existing LoRA entry in the database"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get the existing LoRA
            result = await session.execute(
                select(Lora).where(Lora.filename == lora_filename)
            )
            lora = result.scalar_one_or_none()
            
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            # Update fields if provided
            if "display_name" in lora_data:
                lora.display_name = lora_data["display_name"]
            if "description" in lora_data:
                lora.description = lora_data["description"]
            if "category" in lora_data:
                lora.category = lora_data["category"]
            if "is_nsfw" in lora_data:
                lora.is_nsfw = lora_data["is_nsfw"]
            if "default_scale" in lora_data:
                lora.default_scale = lora_data["default_scale"]
            if "min_scale" in lora_data:
                lora.min_scale = lora_data["min_scale"]
            if "max_scale" in lora_data:
                lora.max_scale = lora_data["max_scale"]
            if "recommended_min" in lora_data:
                lora.recommended_min = lora_data["recommended_min"]
            if "recommended_max" in lora_data:
                lora.recommended_max = lora_data["recommended_max"]
            if "trigger_words" in lora_data:
                lora.trigger_words = lora_data["trigger_words"]
            if "usage_tips" in lora_data:
                lora.usage_tips = lora_data["usage_tips"]
            if "author" in lora_data:
                lora.author = lora_data["author"]
            if "version" in lora_data:
                lora.version = lora_data["version"]
            if "website" in lora_data:
                lora.website = lora_data["website"]
            if "tags" in lora_data:
                lora.tags = lora_data["tags"]
            
            # Update timestamp
            lora.updated_at = func.now()
            
            await session.commit()
            await session.refresh(lora)
            
            return {
                "status": "success",
                "message": f"LoRA '{lora.display_name}' updated successfully",
                "lora": {
                    "id": lora.id,
                    "filename": lora.filename,
                    "display_name": lora.display_name,
                    "category": lora.category.value if hasattr(lora.category, 'value') else str(lora.category)
                }
            }
            
    except Exception as e:
        logger.error(f"Error updating LoRA: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/loras/{lora_filename}")
async def delete_lora(lora_filename: str):
    """Delete a LoRA entry from the database"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get the LoRA
            result = await session.execute(
                select(Lora).where(Lora.filename == lora_filename)
            )
            lora = result.scalar_one_or_none()
            
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            lora_display_name = lora.display_name
            
            # Delete the LoRA (this will cascade delete compatibility relationships)
            await session.delete(lora)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"LoRA '{lora_display_name}' deleted successfully"
            }
            
    except Exception as e:
        logger.error(f"Error deleting LoRA: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/loras/{lora_filename}/compatibility")
async def get_lora_compatibility(lora_filename: str):
    """Get the compatible models for a specific LoRA"""
    try:
        db_client = await get_db_client()
        
        # Get the LoRA from database
        async with db_client.get_session() as session:
            result = await session.execute(
                select(Lora)
                .options(selectinload(Lora.compatible_models).selectinload(LoraCompatibility.model))
                .where(Lora.filename == lora_filename)
            )
            lora = result.scalar_one_or_none()
            
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            compatible_models = []
            for compatibility in lora.compatible_models:
                compatible_models.append({
                    "model_id": compatibility.model.id,
                    "filename": compatibility.model.filename,
                    "display_name": compatibility.model.display_name,
                    "base_model": compatibility.model.base_model.value if hasattr(compatibility.model.base_model, 'value') else str(compatibility.model.base_model)
                })
            
            return {
                "lora_filename": lora_filename,
                "lora_display_name": lora.display_name,
                "compatible_models": compatible_models,
                "total_compatible": len(compatible_models)
            }
            
    except Exception as e:
        logger.error(f"Error getting LoRA compatibility: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/loras/{lora_filename}/compatibility/{model_filename}")
async def add_lora_model_compatibility(lora_filename: str, model_filename: str):
    """Add a compatibility relationship between a LoRA and a model"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            
            # Get the LoRA
            lora_result = await session.execute(
                select(Lora).where(Lora.filename == lora_filename)
            )
            lora = lora_result.scalar_one_or_none()
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            # Get the model
            model_result = await session.execute(
                select(AiModel).where(AiModel.filename == model_filename)
            )
            model = model_result.scalar_one_or_none()
            if not model:
                raise HTTPException(status_code=404, detail="Model not found")
            
            # Check if compatibility already exists
            existing_result = await session.execute(
                select(LoraCompatibility)
                .where(LoraCompatibility.lora_id == lora.id)
                .where(LoraCompatibility.model_id == model.id)
            )
            existing = existing_result.scalar_one_or_none()
            
            if existing:
                return {
                    "status": "exists",
                    "message": f"Compatibility between {lora_filename} and {model_filename} already exists"
                }
            
            # Create compatibility relationship
            compatibility = LoraCompatibility(
                lora_id=lora.id,
                model_id=model.id
            )
            session.add(compatibility)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Added compatibility between {lora.display_name} and {model.display_name}",
                "lora": {"filename": lora.filename, "display_name": lora.display_name},
                "model": {"filename": model.filename, "display_name": model.display_name}
            }
            
    except Exception as e:
        logger.error(f"Error adding LoRA compatibility: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/loras/{lora_filename}/compatibility/{model_filename}")
async def remove_lora_model_compatibility(lora_filename: str, model_filename: str):
    """Remove a compatibility relationship between a LoRA and a model"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            
            # Get the LoRA and model IDs
            lora_result = await session.execute(
                select(Lora).where(Lora.filename == lora_filename)
            )
            lora = lora_result.scalar_one_or_none()
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            model_result = await session.execute(
                select(AiModel).where(AiModel.filename == model_filename)
            )
            model = model_result.scalar_one_or_none()
            if not model:
                raise HTTPException(status_code=404, detail="Model not found")
            
            # Find and delete the compatibility relationship
            compatibility_result = await session.execute(
                select(LoraCompatibility)
                .where(LoraCompatibility.lora_id == lora.id)
                .where(LoraCompatibility.model_id == model.id)
            )
            compatibility = compatibility_result.scalar_one_or_none()
            
            if not compatibility:
                raise HTTPException(status_code=404, detail="Compatibility relationship not found")
            
            await session.delete(compatibility)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Removed compatibility between {lora.display_name} and {model.display_name}"
            }
            
    except Exception as e:
        logger.error(f"Error removing LoRA compatibility: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/loras/{lora_filename}/compatibility/bulk")
async def set_lora_model_compatibility_bulk(lora_filename: str, request: dict):
    """Set all compatible models for a LoRA (replaces existing relationships)"""
    try:
        model_filenames = request.get("model_filenames", [])
        
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            
            # Get the LoRA
            lora_result = await session.execute(
                select(Lora).where(Lora.filename == lora_filename)
            )
            lora = lora_result.scalar_one_or_none()
            if not lora:
                raise HTTPException(status_code=404, detail="LoRA not found")
            
            # Remove all existing compatibilities for this LoRA
            existing_result = await session.execute(
                select(LoraCompatibility).where(LoraCompatibility.lora_id == lora.id)
            )
            existing_compatibilities = existing_result.scalars().all()
            for comp in existing_compatibilities:
                await session.delete(comp)
            
            # Add new compatibilities
            added_models = []
            for model_filename in model_filenames:
                model_result = await session.execute(
                    select(AiModel).where(AiModel.filename == model_filename)
                )
                model = model_result.scalar_one_or_none()
                if model:
                    compatibility = LoraCompatibility(
                        lora_id=lora.id,
                        model_id=model.id
                    )
                    session.add(compatibility)
                    added_models.append({
                        "filename": model.filename,
                        "display_name": model.display_name
                    })
            
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Set {len(added_models)} compatible models for {lora.display_name}",
                "lora": {"filename": lora.filename, "display_name": lora.display_name},
                "compatible_models": added_models
            }
            
    except Exception as e:
        logger.error(f"Error setting bulk LoRA compatibility: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ===== VAE CRUD ENDPOINTS =====

@app.post("/vaes")
async def create_vae(vae_data: dict):
    """Create new VAE"""
    try:
        db_client = await get_db_client()
        
        # Validate required fields
        required_fields = ["filename", "display_name", "file_path"]
        for field in required_fields:
            if field not in vae_data:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        async with db_client.get_session() as session:
            # Check if VAE with filename already exists
            existing_result = await session.execute(
                select(Vae).where(Vae.filename == vae_data["filename"])
            )
            if existing_result.scalar_one_or_none():
                raise HTTPException(status_code=400, detail="VAE with this filename already exists")
            
            # Create VAE
            vae = Vae(
                id=str(uuid.uuid4()),
                filename=vae_data["filename"],
                display_name=vae_data["display_name"],
                description=vae_data.get("description"),
                vae_type=VaeType(vae_data.get("vae_type", "SDXL")),
                file_path=vae_data["file_path"],
                file_size=vae_data.get("file_size"),
                checksum=vae_data.get("checksum"),
                is_active=vae_data.get("is_active", True),
                is_default=vae_data.get("is_default", False),
                author=vae_data.get("author"),
                version=vae_data.get("version"),
                website=vae_data.get("website"),
                source=vae_data.get("source", "local"),
                tags=vae_data.get("tags", []),
                usage_count=0
            )
            
            session.add(vae)
            await session.commit()
            await session.refresh(vae)
            
            return {
                "status": "success",
                "message": f"VAE '{vae_data['display_name']}' created successfully",
                "vae": {
                    "id": vae.id,
                    "filename": vae.filename,
                    "display_name": vae.display_name,
                    "description": vae.description,
                    "vae_type": vae.vae_type.value,
                    "is_active": vae.is_active,
                    "is_default": vae.is_default
                }
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating VAE: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.put("/vaes/{vae_filename}")
async def update_vae(vae_filename: str, vae_data: dict):
    """Update existing VAE"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get VAE
            result = await session.execute(
                select(Vae).where(Vae.filename == vae_filename)
            )
            vae = result.scalar_one_or_none()
            if not vae:
                raise HTTPException(status_code=404, detail="VAE not found")
            
            # Update fields
            updateable_fields = [
                "display_name", "description", "vae_type", "is_active", "is_default",
                "author", "version", "website", "tags"
            ]
            
            for field in updateable_fields:
                if field in vae_data:
                    if field == "vae_type":
                        vae.vae_type = VaeType(vae_data[field])
                    else:
                        setattr(vae, field, vae_data[field])
            
            await session.commit()
            await session.refresh(vae)
            
            return {
                "status": "success", 
                "message": f"VAE '{vae_filename}' updated successfully",
                "vae": {
                    "filename": vae.filename,
                    "display_name": vae.display_name,
                    "description": vae.description,
                    "vae_type": vae.vae_type.value,
                    "is_active": vae.is_active,
                    "is_default": vae.is_default
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating VAE: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.delete("/vaes/{vae_filename}")
async def delete_vae(vae_filename: str):
    """Delete VAE"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get VAE
            result = await session.execute(
                select(Vae).where(Vae.filename == vae_filename)
            )
            vae = result.scalar_one_or_none()
            if not vae:
                raise HTTPException(status_code=404, detail="VAE not found")
            
            # Delete VAE
            await session.delete(vae)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"VAE '{vae_filename}' deleted successfully"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting VAE: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


# ===== UPSCALER CRUD ENDPOINTS =====

@app.post("/upscalers")
async def create_upscaler(upscaler_data: dict):
    """Create new Upscaler"""
    try:
        db_client = await get_db_client()
        
        # Validate required fields
        required_fields = ["filename", "model_name", "file_path", "file_format"]
        for field in required_fields:
            if field not in upscaler_data:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        async with db_client.get_session() as session:
            # Check if Upscaler with filename already exists
            existing_result = await session.execute(
                select(Upscaler).where(Upscaler.filename == upscaler_data["filename"])
            )
            if existing_result.scalar_one_or_none():
                raise HTTPException(status_code=400, detail="Upscaler with this filename already exists")
            
            # Create Upscaler
            upscaler = Upscaler(
                id=str(uuid.uuid4()),
                filename=upscaler_data["filename"],
                model_name=upscaler_data["model_name"],
                description=upscaler_data.get("description"),
                upscaler_type=UpscalerType(upscaler_data.get("upscaler_type", "GENERAL")),
                file_path=upscaler_data["file_path"],
                file_format=upscaler_data["file_format"],
                file_size=upscaler_data.get("file_size"),
                checksum=upscaler_data.get("checksum"),
                is_active=upscaler_data.get("is_active", True),
                scale_factor=upscaler_data.get("scale_factor", 4),
                tile_size=upscaler_data.get("tile_size", 512),
                tile_pad=upscaler_data.get("tile_pad", 10),
                supports_half=upscaler_data.get("supports_half", True),
                author=upscaler_data.get("author"),
                version=upscaler_data.get("version"),
                website=upscaler_data.get("website"),
                source=upscaler_data.get("source", "local"),
                tags=upscaler_data.get("tags", []),
                usage_count=0
            )
            
            session.add(upscaler)
            await session.commit()
            await session.refresh(upscaler)
            
            return {
                "status": "success",
                "message": f"Upscaler '{upscaler_data['model_name']}' created successfully",
                "upscaler": {
                    "id": upscaler.id,
                    "filename": upscaler.filename,
                    "model_name": upscaler.model_name,
                    "description": upscaler.description,
                    "upscaler_type": upscaler.upscaler_type.value,
                    "is_active": upscaler.is_active,
                    "scale_factor": upscaler.scale_factor
                }
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating Upscaler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.put("/upscalers/{upscaler_filename}")
async def update_upscaler(upscaler_filename: str, upscaler_data: dict):
    """Update existing Upscaler"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Upscaler
            result = await session.execute(
                select(Upscaler).where(Upscaler.filename == upscaler_filename)
            )
            upscaler = result.scalar_one_or_none()
            if not upscaler:
                raise HTTPException(status_code=404, detail="Upscaler not found")
            
            # Update fields
            updateable_fields = [
                "model_name", "description", "upscaler_type", "is_active", "scale_factor",
                "tile_size", "tile_pad", "supports_half", "author", "version", "website", "tags"
            ]
            
            for field in updateable_fields:
                if field in upscaler_data:
                    if field == "upscaler_type":
                        upscaler.upscaler_type = UpscalerType(upscaler_data[field])
                    else:
                        setattr(upscaler, field, upscaler_data[field])
            
            await session.commit()
            await session.refresh(upscaler)
            
            return {
                "status": "success", 
                "message": f"Upscaler '{upscaler_filename}' updated successfully",
                "upscaler": {
                    "filename": upscaler.filename,
                    "model_name": upscaler.model_name,
                    "description": upscaler.description,
                    "upscaler_type": upscaler.upscaler_type.value,
                    "is_active": upscaler.is_active,
                    "scale_factor": upscaler.scale_factor
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating Upscaler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.delete("/upscalers/{upscaler_filename}")
async def delete_upscaler(upscaler_filename: str):
    """Delete Upscaler"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Upscaler
            result = await session.execute(
                select(Upscaler).where(Upscaler.filename == upscaler_filename)
            )
            upscaler = result.scalar_one_or_none()
            if not upscaler:
                raise HTTPException(status_code=404, detail="Upscaler not found")
            
            # Delete Upscaler
            await session.delete(upscaler)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Upscaler '{upscaler_filename}' deleted successfully"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting Upscaler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


# ===== SAMPLER CRUD ENDPOINTS =====

@app.post("/samplers")
async def create_sampler(sampler_data: dict):
    """Create new Sampler"""
    try:
        db_client = await get_db_client()
        
        # Validate required fields
        required_fields = ["name", "display_name", "scheduler_class", "config"]
        for field in required_fields:
            if field not in sampler_data:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        async with db_client.get_session() as session:
            # Check if Sampler with name already exists
            existing_result = await session.execute(
                select(Sampler).where(Sampler.name == sampler_data["name"])
            )
            if existing_result.scalar_one_or_none():
                raise HTTPException(status_code=400, detail="Sampler with this name already exists")
            
            # Create Sampler
            sampler = Sampler(
                id=str(uuid.uuid4()),
                name=sampler_data["name"],
                display_name=sampler_data["display_name"],
                description=sampler_data.get("description"),
                scheduler_class=sampler_data["scheduler_class"],
                config=sampler_data["config"],
                is_active=sampler_data.get("is_active", True),
                is_default=sampler_data.get("is_default", False),
                speed_rating=sampler_data.get("speed_rating", 3),
                quality_rating=sampler_data.get("quality_rating", 3),
                usage_count=0
            )
            
            session.add(sampler)
            await session.commit()
            await session.refresh(sampler)
            
            return {
                "status": "success",
                "message": f"Sampler '{sampler_data['display_name']}' created successfully",
                "sampler": {
                    "id": sampler.id,
                    "name": sampler.name,
                    "display_name": sampler.display_name,
                    "description": sampler.description,
                    "scheduler_class": sampler.scheduler_class,
                    "is_active": sampler.is_active,
                    "is_default": sampler.is_default
                }
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating Sampler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.put("/samplers/{sampler_name}")
async def update_sampler(sampler_name: str, sampler_data: dict):
    """Update existing Sampler"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Sampler
            result = await session.execute(
                select(Sampler).where(Sampler.name == sampler_name)
            )
            sampler = result.scalar_one_or_none()
            if not sampler:
                raise HTTPException(status_code=404, detail="Sampler not found")
            
            # Update fields
            updateable_fields = [
                "display_name", "description", "scheduler_class", "config", "is_active", 
                "is_default", "speed_rating", "quality_rating"
            ]
            
            for field in updateable_fields:
                if field in sampler_data:
                    setattr(sampler, field, sampler_data[field])
            
            await session.commit()
            await session.refresh(sampler)
            
            return {
                "status": "success", 
                "message": f"Sampler '{sampler_name}' updated successfully",
                "sampler": {
                    "name": sampler.name,
                    "display_name": sampler.display_name,
                    "description": sampler.description,
                    "is_active": sampler.is_active,
                    "is_default": sampler.is_default
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating Sampler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.delete("/samplers/{sampler_name}")
async def delete_sampler(sampler_name: str):
    """Delete Sampler"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Sampler
            result = await session.execute(
                select(Sampler).where(Sampler.name == sampler_name)
            )
            sampler = result.scalar_one_or_none()
            if not sampler:
                raise HTTPException(status_code=404, detail="Sampler not found")
            
            # Delete Sampler
            await session.delete(sampler)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Sampler '{sampler_name}' deleted successfully"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting Sampler: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


# ===== SYSTEM CONFIG CRUD ENDPOINTS =====

@app.get("/config")
async def list_config():
    """List all system configuration"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            result = await session.execute(select(SystemConfig))
            configs = result.scalars().all()
        
            return {
                "configs": [
                    {
                        "id": config.id,
                        "key": config.key,
                        "value": config.value if not config.is_secret else "[HIDDEN]",
                        "type": config.type,
                        "description": config.description,
                        "category": config.category,
                        "is_secret": config.is_secret
                    }
                    for config in configs
                ],
                "total_count": len(configs)
            }
        
    except Exception as e:
        logger.error(f"Error listing config: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/config")
async def create_config(config_data: dict):
    """Create new system configuration"""
    try:
        db_client = await get_db_client()
        
        # Validate required fields
        required_fields = ["key", "value"]
        for field in required_fields:
            if field not in config_data:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        async with db_client.get_session() as session:
            # Check if config with key already exists
            existing_result = await session.execute(
                select(SystemConfig).where(SystemConfig.key == config_data["key"])
            )
            if existing_result.scalar_one_or_none():
                raise HTTPException(status_code=400, detail="Config with this key already exists")
            
            # Create Config
            config = SystemConfig(
                id=str(uuid.uuid4()),
                key=config_data["key"],
                value=config_data["value"],
                type=config_data.get("type", "string"),
                description=config_data.get("description"),
                category=config_data.get("category"),
                is_secret=config_data.get("is_secret", False)
            )
            
            session.add(config)
            await session.commit()
            await session.refresh(config)
            
            return {
                "status": "success",
                "message": f"Config '{config_data['key']}' created successfully",
                "config": {
                    "key": config.key,
                    "value": config.value if not config.is_secret else "[HIDDEN]",
                    "type": config.type,
                    "description": config.description,
                    "category": config.category,
                    "is_secret": config.is_secret
                }
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating config: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.put("/config/{config_key}")
async def update_config(config_key: str, config_data: dict):
    """Update existing system configuration"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Config
            result = await session.execute(
                select(SystemConfig).where(SystemConfig.key == config_key)
            )
            config = result.scalar_one_or_none()
            if not config:
                raise HTTPException(status_code=404, detail="Config not found")
            
            # Update fields
            updateable_fields = ["value", "type", "description", "category", "is_secret"]
            
            for field in updateable_fields:
                if field in config_data:
                    setattr(config, field, config_data[field])
            
            await session.commit()
            await session.refresh(config)
            
            return {
                "status": "success", 
                "message": f"Config '{config_key}' updated successfully",
                "config": {
                    "key": config.key,
                    "value": config.value if not config.is_secret else "[HIDDEN]",
                    "type": config.type,
                    "description": config.description,
                    "category": config.category,
                    "is_secret": config.is_secret
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating config: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.delete("/config/{config_key}")
async def delete_config(config_key: str):
    """Delete system configuration"""
    try:
        db_client = await get_db_client()
        
        async with db_client.get_session() as session:
            # Get Config
            result = await session.execute(
                select(SystemConfig).where(SystemConfig.key == config_key)
            )
            config = result.scalar_one_or_none()
            if not config:
                raise HTTPException(status_code=404, detail="Config not found")
            
            # Delete Config
            await session.delete(config)
            await session.commit()
            
            return {
                "status": "success",
                "message": f"Config '{config_key}' deleted successfully"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting config: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    logger.info("ðŸš€ Starting Image Generation API v2.0.0")
    logger.info("   âœ… SDXL model support (non-gated models only)")
    logger.info("   âœ… Semantic chunking for unlimited prompt lengths")
    logger.info("   âœ… Real FP8 precision for RTX 5090")
    logger.info("   âœ… Maximum RTX 5090 optimizations")
    logger.info("   âœ… AI upscaling support (.pt/.pth/.safetensors)")
    logger.info("   âœ… Base model compatibility filtering for LoRAs")
    uvicorn.run(app, host="0.0.0.0", port=8000) 